{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import tflearn\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('middleresult/en_ch_35word.pkl','wb') as whdl:\n",
    "    pickle.dump((\n",
    "        train_x,\n",
    "        test_x,\n",
    "        train_y,\n",
    "        test_y,\n",
    "        ind2ch,\n",
    "        ch2ind,\n",
    "        ind2en,\n",
    "        en2ind,\n",
    "    ),whdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/preprocessing_tokenlizer/sentence_tokened_by_word.pkl','rb') as fhdl:\n",
    "    (\n",
    "         ind2ch,\n",
    "         ch2ind,\n",
    "         ind2en,\n",
    "         en2ind,\n",
    "         train_x,\n",
    "         train_y,\n",
    "    ) = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/preprocessing_subword/subwords_allwords.en','rb') as fhdl:\n",
    "    en_subword = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_subword_dic = dict(zip(en_subword['origin'],en_subword['segmented']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('middleresult/char/zh_vocab.txt',encoding='utf-8') as fhdl:\n",
    "    ch_subwords = [line.strip().split(\"\\t\") for line in fhdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661850, 406740)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind2ch),len(ind2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_inv_size_base = 20000#len(ind2en) + 3\n",
    "target_inv_size_base = 20000#len(ind2ch) + 3\n",
    "\n",
    "USE_GPU = 0\n",
    "\n",
    "attention_hidden_size = 1024\n",
    "attention_output_size = 1024\n",
    "embedding_size = 1024\n",
    "seq_max_len = 60\n",
    "num_units = 1024\n",
    "batch_size = 64\n",
    "layer_number = 2\n",
    "max_grad = 1.0\n",
    "dropout = 0.2\n",
    "sentence_max_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_inv = list(map(lambda x:x[0],sorted(ch2ind.items(),key=lambda x:x[1])[:target_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_inv = list(map(lambda x:x[0],sorted(en2ind.items(),key=lambda x:x[1])[:src_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_inv = en_inv[:3] + ['_' + i for i in en_inv[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_inv_tmpdic = dict(zip(ch_inv,range(len(ch_inv))))\n",
    "ch_oov = [i[0] for i in ch_subwords if i[0] not in ch_inv_tmpdic]\n",
    "en_inv_tmpdic = dict(zip(en_inv,range(len(en_inv))))\n",
    "en_oov = [i for i in en_subword['subwords'] if i not in en_inv_tmpdic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2ch_oov = dict(zip(range(len(ch_oov) + len(ch_inv)),ch_inv + ch_oov))\n",
    "ch2ind_oov = dict(zip(ch_inv + ch_oov,range(len(ch_oov) + len(ch_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2en_oov = dict(zip(range(len(en_oov) + len(en_inv)),en_inv + en_oov))\n",
    "en2ind_oov = dict(zip(en_inv + en_oov,range(len(en_oov) + len(en_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6502, 3758)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_oov),len(en_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = src_inv_size_base + len(en_oov)\n",
    "target_vocat_size = target_inv_size_base + len(ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23758, 23758, 26502, 26502)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size,len(ind2en_oov),target_vocat_size,len(ind2ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 10000000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360, 2360)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ind['james'],en2ind_oov['_james']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = [i[::-1] for i in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = sequence.pad_sequences(train_x,seq_max_len,padding='post',value=en2ind['<eos>'])\n",
    "#train_y = sequence.pad_sequences(train_y,seq_max_len,padding='post',value=ch2ind['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have met your fianc é e , i believe .\n",
      "不过 ， 我 肯定 见 过 您 的 未婚妻 。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0,len(train_x))\n",
    "print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_x,train_y , test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def en_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < src_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            en_word = ind2en[wordind]\n",
    "            if en_word not in en_subword_dic:\n",
    "                sentence_inds.append(en2ind_oov['<unk>'])\n",
    "            en_pieces = en_subword_dic[en_word]\n",
    "            pieces_index = [en2ind_oov[i] for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ch_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < target_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            ch_word = ind2ch[wordind]\n",
    "            en_pieces = [i for i in ch_word]\n",
    "            pieces_index = [ch2ind_oov.get(i,ch2ind_oov['<unk>']) for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_tmp = []\n",
    "for index,sentence in enumerate(train_x):\n",
    "    train_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_tmp = []\n",
    "for index,sentence in enumerate(train_y):\n",
    "    train_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_tmp = []\n",
    "for index,sentence in enumerate(test_x):\n",
    "    test_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_tmp = []\n",
    "for index,sentence in enumerate(test_y):\n",
    "    test_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x_tmp)\n",
    "train_y = np.asarray(train_y_tmp)\n",
    "test_x = np.asarray(test_x_tmp)\n",
    "test_y = np.asarray(test_y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x_tmp\n",
    "del train_y_tmp\n",
    "del test_x_tmp\n",
    "del test_y_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_shall _we _take _the _ferry _? _i _' _ve _never _taken _one _.\n",
      "我们 能 不能 坐坐 那个 渡轮 ？ 我 从来 没 坐 过 呢 。\n",
      "_and _i _don _' _t _think _i _have _to _get _permission _from _nobody _for _nothing _.\n",
      "我 不 需要 从 任何人 那里 得 到 许 可 。\n",
      "_i _' _m _an _american _. _i _don _' _t _share _your _english _hatred _of _comfort _.\n",
      "我 是 美国 人 ， 不像 你们 英国人 还会 仇 视 舒适 。\n",
      "_i _' _m _trying _to _give _him _a _second _chance _.\n",
      "我 想 给 他 第二次 机会 。\n",
      "_the _allegations _will _bring _british _parliamentary _government _into _dis rep ute _.\n",
      "这些 说法 将 使 英国 的 议会 政治 名 誉 扫 地 。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(train_x))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in train_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:{}'.format(USE_GPU)):\n",
    "    #initializer = tf.random_uniform_initializer(\n",
    "    #    -0.08, 0.08)\n",
    "    initializer = tf.truncated_normal_initializer(\n",
    "        mean=0.0,stddev=0.02)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    x_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_max_len = tf.placeholder(tf.int32, shape=[])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "    \n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # encoder\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    cell_list = []\n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "    cell_list = []\n",
    "    \n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_backword_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_backword_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell,encoder_backword_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len,dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        encoder_state = tuple(encoder_state)\n",
    "    \n",
    "    # decoder \n",
    "    #decoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    cell_list = []\n",
    "    for i in range(layer_number):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        decoder_cell = cell_list[0]\n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    # Helper\n",
    "    \n",
    "    # attention\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        attention_hidden_size, encoder_outputs,\n",
    "        memory_sequence_length=x_real_len,scale=True)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=attention_output_size)\n",
    "    \n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        target_vocat_size, use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp,sequence_length= y_len)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "       \n",
    "        outputs, _,___  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_real_len, y_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        helper_predict = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_decoder,\n",
    "            tf.fill([batch_size], ch2ind['<go>']), ch2ind['<eos>'])\n",
    "        decoder_predict = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper_predict, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "        outputs_predict,_, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_predict, maximum_iterations=sentence_max_length)\n",
    "    translations = outputs_predict.sample_id\n",
    "\n",
    "    # calculate loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        batch_size)\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(logits,target):\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target[:,:seq_max_len], logits[:,:seq_max_len]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saver.restore(session,'middleresult/align/result_1_20847')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middleresult/result_deep'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'middleresult/result_deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saver.save(session,'middleresult/result_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import Dataset,ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bleu_score(predict,target):\n",
    "    try:\n",
    "        target = [[[j for index,j in enumerate(i)]] for i in target]\n",
    "        predict = [[j for index,j in enumerate(i)] for i in predict]\n",
    "        BLEUscore = nltk.translate.bleu_score.corpus_bleu(target,predict)\n",
    "    except:\n",
    "        BLEUscore = -1\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x),len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_len = [len(i) for i in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = list(filter(lambda x:x[2] < 50,sorted(zip(test_x,test_y,test_x_len),key=lambda x:x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.asarray([i[0] for i in tmp])\n",
    "test_y = np.asarray([i[1] for i in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_how _many _prints _?\n",
      "多少 指纹 ？\n",
      "_see _, _different _frequencies\n",
      "明白 ， 没 不同 的 频繁\n",
      "_telling _us _:\n",
      "告诉 我们 ：\n",
      "_villa _room\n",
      "别墅 房\n",
      "_pioneer _day _!\n",
      "先 驱 者 日 ！\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(test_x[:1500]))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in test_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in test_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99263, 99263)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_test_loss(test_x,test_y,display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(0,len(test_x),batch_size):\n",
    "        batch_x,batch_y = test_x[j:j + batch_size],test_y[j:j + batch_size]#test_set.next_batch(batch_size)\n",
    "        if len(batch_x) < batch_size:\n",
    "            continue\n",
    "        bx = [len(m) + 1 for m in batch_x]\n",
    "        by = [len(m) + 1 for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        \n",
    "        tmp_loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,\n",
    "                                                     y_in:\n",
    "                                                     np.concatenate((\n",
    "                                                     np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                     ,x_len:lx,y_len:ly,\n",
    "                                                                        y_real_len:by,\n",
    "                                                                        x_real_len:bx,\n",
    "                                                                        y_max_len:max(by)\n",
    "                                                                        })\n",
    "        loss_list.append(tmp_loss)\n",
    "        tmp_acc = cal_acc(tran,batch_y)\n",
    "        accs.append(tmp_acc)\n",
    "        predict_list += [i for i in tran]\n",
    "        target_list += [i for i in batch_y]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    return np.average(loss_list),np.average(accs),get_bleu_score(predict_list,target_list),predict_list,target_list,source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 15/15 \t used:21s eta:0 s"
     ]
    }
   ],
   "source": [
    "w_loss,w_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(train_x[::10000],train_y[::10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134.038 0.0 0.08783602619713961\n"
     ]
    }
   ],
   "source": [
    "print(w_loss,w_acc,bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_text(x):\n",
    "    return [' '.join([ind2ch_oov.get(j,'') for j in i]) for i in x]\n",
    "def get_all_en_text(x):\n",
    "    return [' '.join([ind2en_oov.get(j,'') for j in i]) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "#                                                                np.concatenate((\n",
    "#                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "#                                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tran.shape\n",
    "i_save = 0\n",
    "j_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(i_save,j_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'OOVSUB-20kword'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('middleresult/{}'.format(model_path)):os.mkdir('middleresult/{}'.format(model_path))\n",
    "if not os.path.exists('eval/{}'.format(model_path)):os.mkdir('eval/{}'.format(model_path))\n",
    "if not os.path.exists('val/{}'.format(model_path)):os.mkdir('val/{}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tmpindexs(train_index_set):\n",
    "    tmp_indexs,_ = train_index_set.next_batch(batch_size * batch_gen)\n",
    "    tmp_length = [len(train_x[i]) for i in tmp_indexs]\n",
    "    tmp_indexs = [i[0] for i in sorted(zip(tmp_indexs,tmp_length),key=lambda x:x[1])]\n",
    "    tmp = []\n",
    "    for i in random.sample(range(batch_gen),batch_gen):\n",
    "        tmp += tmp_indexs[i * batch_size:(i + 1) * batch_size]\n",
    "    tmp_indexs = tmp\n",
    "    return tmp_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_indexs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 154/154 \t used:96s eta:0 ss1/154687 \t used:12115s eta:36346 s\n",
      "iter 1 step 38671 train loss 64.45343780517578 train acc 0.4313197327802754 test loss 51.75115203857422 test acc 0.22917216220119419 bleu 0.09593322531527192 lr 0.5\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:93s eta:0 ss2/154687 \t used:24422s eta:24422 s\n",
      "iter 1 step 77342 train loss 49.80830764770508 train acc 0.38937694139441037 test loss 45.17483139038086 test acc 0.2672422091404579 bleu 0.12050399382710016 lr 0.5\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:92s eta:0 s3/154687 \t used:36711s eta:12237 sss\n",
      "iter 1 step 116013 train loss 45.005558013916016 train acc 0.33190713057211635 test loss 41.782596588134766 test acc 0.25812319086047175 bleu 0.1018826761959946 lr 0.5\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:85s eta:0 s684/154687 \t used:48962s eta:0 sssss\n",
      "iter 1 step 154684 train loss 42.22779083251953 train acc 0.34065841547465386 test loss 39.608741760253906 test acc 0.25541988662160375 bleu 0.09344880480842628 lr 0.5\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:84s eta:0 s71/154687 \t used:12060s eta:36181 s\n",
      "iter 2 step 38671 train loss 40.104766845703125 train acc 0.36763982452952115 test loss 38.005775451660156 test acc 0.2620213981923971 bleu 0.10950288969134081 lr 0.5\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:82s eta:0 s2/154687 \t used:24317s eta:24317 ss\n",
      "iter 2 step 77342 train loss 38.742919921875 train acc 0.34960638603602195 test loss 36.8120002746582 test acc 0.26412406964569735 bleu 0.12063799740791278 lr 0.5\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:84s eta:0 s013/154687 \t used:36567s eta:12189 ss\n",
      "iter 2 step 116013 train loss 37.14163589477539 train acc 0.34046405625254994 test loss 35.533206939697266 test acc 0.2672869336447815 bleu 0.11649136144386595 lr 0.25\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:84s eta:0 s4684/154687 \t used:48822s eta:0 sssss\n",
      "iter 2 step 154684 train loss 36.5228157043457 train acc 0.35324868100569856 test loss 34.98558807373047 test acc 0.2772369485084275 bleu 0.13465585114078957 lr 0.25\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:83s eta:0 s8671/154687 \t used:12294s eta:36883 s\n",
      "iter 3 step 38671 train loss 35.37446594238281 train acc 0.3612033685838283 test loss 34.412166595458984 test acc 0.2728891023791267 bleu 0.13126292697660183 lr 0.125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:83s eta:0 s7342/154687 \t used:24816s eta:24816 s\n",
      "iter 3 step 77342 train loss 35.108158111572266 train acc 0.33554000499568165 test loss 34.17097091674805 test acc 0.2714071405435281 bleu 0.1313267455344315 lr 0.125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:81s eta:0 s116013/154687 \t used:37395s eta:12465 s\n",
      "iter 3 step 116013 train loss 34.812355041503906 train acc 0.3586933581768829 test loss 33.91442108154297 test acc 0.2753335434186256 bleu 0.13675649154818714 lr 0.0625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:82s eta:0 s154684/154687 \t used:49978s eta:0 sssss\n",
      "iter 3 step 154684 train loss 34.67301559448242 train acc 0.3439584904366086 test loss 33.752262115478516 test acc 0.2770499546878705 bleu 0.13866488434373095 lr 0.0625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:83s eta:0 s38671/154687 \t used:12384s eta:37151 ss\n",
      "iter 4 step 38671 train loss 34.26351547241211 train acc 0.3570289466520635 test loss 33.63890075683594 test acc 0.2773458994131058 bleu 0.1360692019581738 lr 0.03125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:81s eta:0 s7342/154687 \t used:24685s eta:24685 sss\n",
      "iter 4 step 77342 train loss 34.22283935546875 train acc 0.3434129023153634 test loss 33.5870361328125 test acc 0.2733719794358009 bleu 0.12924812127831944 lr 0.03125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:81s eta:0 s 116013/154687 \t used:37016s eta:12339 ss\n",
      "iter 4 step 116013 train loss 34.22969436645508 train acc 0.3493099550716213 test loss 33.53103256225586 test acc 0.27551334670010424 bleu 0.13727329470527466 lr 0.015625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:83s eta:0 s 154684/154687 \t used:49338s eta:0 ssssss\n",
      "iter 4 step 154684 train loss 34.12060546875 train acc 0.3532196844959378 test loss 33.47365188598633 test acc 0.2777647363121503 bleu 0.1362548291713786 lr 0.015625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:82s eta:0 s-] 38671/154687 \t used:12316s eta:36948 s\n",
      "iter 5 step 38671 train loss 34.01007080078125 train acc 0.35042299721531545 test loss 33.468017578125 test acc 0.27661653665821867 bleu 0.1385345339641979 lr 0.0078125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:81s eta:0 s-] 77342/154687 \t used:24835s eta:24835 s\n",
      "iter 5 step 77342 train loss 33.98850631713867 train acc 0.34586330838169155 test loss 33.448848724365234 test acc 0.27547352243873474 bleu 0.13790860216634296 lr 0.0078125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:81s eta:0 s-] 116013/154687 \t used:37382s eta:12461 ss\n",
      "iter 5 step 116013 train loss 34.02283477783203 train acc 0.34978948696725604 test loss 33.41532897949219 test acc 0.2734044835519758 bleu 0.13439666824525723 lr 0.00390625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:82s eta:0 s>-] 154684/154687 \t used:49908s eta:0 sssss\n",
      "iter 5 step 154684 train loss 33.98800277709961 train acc 0.35416619586735415 test loss 33.4040412902832 test acc 0.2750048945208155 bleu 0.1363567306928264 lr 0.00390625\n",
      "\n",
      "iter 6 loss:34.0876942741517 lr:0.001953125 23.98 % [===========>---------------------------------------] 37090/154687 \t used:11800s eta:37413 s s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-0118dba81b33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                                                 \u001b[1;33m,\u001b[0m\u001b[0my_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                                                 \u001b[0mx_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                                                 \u001b[0my_max_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                                                                })\n\u001b[1;32m     53\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 60\n",
    "restore = False\n",
    "lr = 1 / 2\n",
    "\n",
    "batch_gen = 100\n",
    "\n",
    "from utils import *\n",
    "if not restore:\n",
    "    train_index_set = Dataset(np.arange(len(train_x)),np.arange(len(train_y)))\n",
    "    tmp_indexs = []\n",
    "    \n",
    "exp_loss = None\n",
    "alpha = 0.97\n",
    "for i in range(i_save,n_epoch):\n",
    "    one_epoch = i + 1\n",
    "    i_save = i\n",
    "    worksum = int(len(train_y)/batch_size)\n",
    "    pb = ProgressBar(worksum=worksum)\n",
    "    pb.startjob()\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for j in range(worksum):\n",
    "        one_batch = j\n",
    "        if restore == True and j < j_save:\n",
    "            pb.finishsum += 1\n",
    "            continue\n",
    "        restore = False\n",
    "        \n",
    "        j_save = j\n",
    "        \n",
    "        if tmp_indexs == []:\n",
    "            tmp_indexs = get_tmpindexs(train_index_set)\n",
    "        batch_indexs,tmp_indexs = tmp_indexs[:batch_size],tmp_indexs[batch_size:]\n",
    "        batch_x,batch_y = train_x[batch_indexs],train_y[batch_indexs]\n",
    "\n",
    "        bx = [min(len(m) + 1,seq_max_len) for m in batch_x]\n",
    "        by = [min(len(m) + 1,seq_max_len) for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        #print(batch_x.shape,batch_y.shape)\n",
    "        \n",
    "        _, loss = session.run([optimizer,train_loss],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "                                                                np.concatenate((\n",
    "                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                                ,y_real_len:by,\n",
    "                                                                x_real_len:bx,\n",
    "                                                                y_max_len:max(by)\n",
    "                                                               })\n",
    "        train_loss_list.append(loss)\n",
    "        #tmp_train_acc = cal_acc(tran,batch_y)\n",
    "        #train_acc_list.append(tmp_train_acc)\n",
    "        exp_loss = loss if exp_loss == None else alpha * exp_loss + (1 - alpha) * loss\n",
    "        pb.info = \"iter {} loss:{} lr:{}\".format(i + 1,exp_loss,lr)\n",
    "        with open('val/{}/train_loss.txt'.format(model_path),'a') as whdl:\n",
    "            whdl.write(\"{}\\t{}\\t{}\\n\".format(one_epoch,one_batch,loss))\n",
    "        val_step = int(worksum / 4)\n",
    "        if j % val_step == 0 and j != 0:\n",
    "            test_loss,test_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(test_x[::4],test_y[::4])\n",
    "            _,train_acc,train_bleu_score,train_predict_list,train_target_list,train_source_list = calc_test_loss(train_x[::1000],train_y[::1000])\n",
    "            predict_texts = get_all_text(predict_list)\n",
    "            target_texts = get_all_text(target_list)\n",
    "            source_texts = get_all_en_text(source_list)\n",
    "            \n",
    "            train_predict_texts = get_all_text(train_predict_list)\n",
    "            train_target_texts = get_all_text(train_target_list)\n",
    "            train_source_texts = get_all_en_text(train_source_list)\n",
    "            \n",
    "            with open('eval/{}/{}_{}_predict'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "                    \n",
    "            with open('eval/{}/{}_{}_predict_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            print(\"\\niter {} step {} train loss {} train acc {} test loss {} test acc {} bleu {} lr {}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            with open('val/{}/test_loss.txt'.format(model_path),'a') as whdl:\n",
    "                whdl.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            try:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session,'middleresult/{}/result_{}_{}'.format(model_path,i + 1,j))\n",
    "            except:\n",
    "                print('save fail')\n",
    "        lr_step = int(worksum / 2) - 1\n",
    "        if j % lr_step == 0 and j != 0:\n",
    "            if (i + 1) >= 2:\n",
    "                lr = lr / 2\n",
    "        pb.complete(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# predict the real shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from middleresult/OOVSUB/result_3_116013\n"
     ]
    }
   ],
   "source": [
    "saver.restore(session,'middleresult/OOVSUB/result_3_116013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from xml.etree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_lines = open('./ai_challenger_translation_test_a_20170923/ai_challenger_translation_test_a_20170923.sgm',encoding='utf-8').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<seg id=\"283\"> The room includes two 32-inch flat screen TV's, sink and microwave & refrigerator. </seg>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 13)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(validation_lines[286]),validation_lines[286].find('>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_text = []\n",
    "for line in validation_lines[4:-4]:\n",
    "    firstpos = line.find('>')\n",
    "    lastpos = -6\n",
    "    validate_text.append(line[firstpos + 1:lastpos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\jiaohuan\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.917 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "for index,val in enumerate(validate_text):\n",
    "    validate_text[index] = [i.lower() for i in jieba.cut(val) if i.strip()]a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'room', 'includes', 'two', '32', '-', 'inch', 'flat', 'screen', 'tv', \"'\", 's', ',', 'sink', 'and', 'microwave', '&', 'refrigerator', '.']\n"
     ]
    }
   ],
   "source": [
    "print(validate_text[282])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_x = []\n",
    "for index,sentence in enumerate(validate_text):\n",
    "    validate_x.append([en2ind.get(word,en2ind['<unk>']) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_x = np.array(validate_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_x_tmp = []\n",
    "for index,sentence in enumerate(validate_x):\n",
    "    validate_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_i _leave _in _the _morning _. _i _work _three _days _. _call _me _.\n",
      "_here _' _s _to _ant ig ra v ity _. _stay _with _me _, _buddy _. _you _' _ll _get _the _spins _.\n",
      "_i _hear _they _send _people _to _cut _you _off _at _the _knees\n",
      "_but _the _secret _is _, _you _grind _it _from _beans _, _not _crap _.\n",
      "_hey _, _where _can _a _man _and _his _missus _get _something _to _eat _around _here _?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(validate_x_tmp))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in validate_x_tmp[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_x = np.asarray(validate_x_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([np.sum(np.asarray(np.asarray(x) == en2ind['<unk>'],dtype=np.int)) for x in validate_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_translation(test_x,display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(0,len(test_x),batch_size):\n",
    "        batch_x = test_x[j:j + batch_size]\n",
    "        while len(batch_x) < batch_size:\n",
    "            batch_x = np.concatenate([batch_x,batch_x],axis=0)\n",
    "        batch_x = batch_x[:batch_size]\n",
    "        bx = [len(m) + 1 for m in batch_x]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        \n",
    "        tran = session.run(translations,feed_dict={x:batch_x\n",
    "                                                     ,x_len:lx,\n",
    "                                                     x_real_len:bx,\n",
    "                                                     })\n",
    "        predict_list += [i for i in tran]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    predict_list = predict_list[:len(test_x)]\n",
    "    return predict_list,source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 125/125 \t used:36s eta:0 s"
     ]
    }
   ],
   "source": [
    "validate_predict,___ = get_translation(validate_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_predict_tmp = []\n",
    "for line in validate_predict:\n",
    "    tmp = []\n",
    "    for word in line:\n",
    "        if word == ch2ind['<eos>']:\n",
    "            break\n",
    "        tmp.append(word)\n",
    "    validate_predict_tmp.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_predict_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_predict = validate_predict_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420\n",
      "_oh _, _sure _, _all _we _have _to _do _is _escape _from _this _cell _, _right _?\n",
      "当然，我们只要从这个牢房里逃走，对吗？\n",
      "3787\n",
      "_i _hope _you _' _re _not _going _to _keep _on _about _it _.\n",
      "希望你不会继续谈这件事。\n",
      "4887\n",
      "_the _germans _do _not _act _like _us _, _neither _do _they _think _like _us _...\n",
      "德国人不像我们一样，他们也不像我们这样的人…\n",
      "7890\n",
      "_if _that _was _my _wife _, _i _know _where _i _' _d _be _.\n",
      "如果那是我妻子，我知道我在哪里。\n",
      "4350\n",
      "_atop _the _pantheon _of _explorers _! _he _' _d _be _able _to _experience\n",
      "在探险家的神殿里！他将能体验到\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(validate_predict))\n",
    "    print(index)\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in validate_x[index]]))\n",
    "    print(''.join([ind2ch_oov.get(i,'') for i in validate_predict[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_result = []\n",
    "for index in range(len(validate_predict)):\n",
    "    one = ''.join([ind2ch_oov.get(i,'') for i in validate_predict[index]])\n",
    "    validate_result.append(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['所以我不会在你面前谈论这个。',\n",
       " '与那些可以帮助他们和金钱的人联系的人联系。',\n",
       " '已经在他们身上了。早一点。来吧。坐下。',\n",
       " '在对酸性反应的反应中变氧化。',\n",
       " '我知道，我也希望能证明，但我不能。',\n",
       " '我不知道！他表现得很搞笑然后开始咳血。',\n",
       " '几位工作人员的成员都可以确认我在那里',\n",
       " '我很高兴你能来陪我，亲爱的。',\n",
       " '先生，这是警察行动。我要谈谈',\n",
       " '很明显是那个撞方向盘的人。让我猜猜。那父亲是喝醉了。']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_result[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('answer/9-26.txt','w',encoding='utf-8') as whdl:\n",
    "    for line in validate_result:\n",
    "        whdl.write(\"{}\\n\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
