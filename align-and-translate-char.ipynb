{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('middleresult/en_ch_35word.pkl','wb') as whdl:\n",
    "    pickle.dump((\n",
    "        train_x,\n",
    "        test_x,\n",
    "        train_y,\n",
    "        test_y,\n",
    "        ind2ch,\n",
    "        ch2ind,\n",
    "        ind2en,\n",
    "        en2ind,\n",
    "    ),whdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('middleresult/tokenlizer_output_60000ch_60000en_40words_token.pkl','rb') as fhdl:\n",
    "    (\n",
    "         ind2ch,\n",
    "         ch2ind,\n",
    "         ind2en,\n",
    "         en2ind,\n",
    "         train_x,\n",
    "         train_y,\n",
    "    ) = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8823, 60000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind2ch),len(ind2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 50003#len(ind2en) + 3\n",
    "target_vocat_size = 8826#len(ind2ch) + 3\n",
    "attention_hidden_size = 1024\n",
    "attention_output_size = 1024\n",
    "embedding_size = 1024\n",
    "seq_max_len = 40\n",
    "num_units = 1024\n",
    "batch_size = 64\n",
    "layer_number = 2\n",
    "max_grad = 1.0\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1905944, 1905944)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = [i[::-1] for i in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = tf.contrib.keras.preprocessing.sequence.pad_sequences(train_x,seq_max_len,padding='post')\n",
    "train_y = tf.contrib.keras.preprocessing.sequence.pad_sequences(train_y,seq_max_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they wanted to see if people would share the video using bluetooth wireless technology .                         \n",
      "他 们 想 看 看 ， 人 们 是 否 会 使 用 蓝 牙 无 线 技 术 分 享 这 个 视 频 。              \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0,len(train_x))\n",
    "print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1925196, 40), (1925196, 40))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_x,train_y , test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x[train_x >= src_vocab_size] = 1 \n",
    "test_x[test_x >= src_vocab_size] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y[train_y >= target_vocat_size] = 1 \n",
    "test_y[test_y >= target_vocat_size] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_last_index = np.max(train_x)\n",
    "y_last_index = np.max(train_y)\n",
    "\n",
    "ind2en[x_last_index] = '<go>'\n",
    "ind2ch[y_last_index] = '<go>'\n",
    "en2ind['<go>'] = x_last_index\n",
    "ch2ind['<go>'] = y_last_index\n",
    "\n",
    "ind2en[1] = '<unk>'\n",
    "ind2ch[1] = '<unk>'\n",
    "en2ind['<unk>'] = 1\n",
    "ch2ind['<unk>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the most recent , the permian , was thought to have been an impact                          \n",
      "最 近 一 次 ， 也 就 是 二 叠 纪 生 物 灭 绝 事 件 被 认 为 是 撞 击 的 结 果 ，             \n",
      "a vulnerability in the default installation of the pl / sql package allows attackers to perform cross - site scripting attacks using the <unk> . print function .            \n",
      "l 包 中 存 在 一 个 漏 洞 ， 使 得 攻 击 者 能 够 使 用 h t p . p r i n t 函 数 执 行 跨 站 点 脚 本 攻 击 。\n",
      "proponents argue that it is wrong to force anyone to contribute money to a union .                        \n",
      "这 也 是 美 国 传 统 工 业 带 第 一 个 通 过 此 法 的 州 。                    \n",
      "many people dream about living on an island in the south seas .                           \n",
      "许 多 人 向 往 在 南 海 的 一 个 岛 上 生 活 。                        \n",
      "( b ) instead of ordering the grant of a licence to the applicant , order the existing licence to be amended .                 \n",
      "( b ) 可 命 令 将 现 有 的 特 许 修 订 ， 而 非 命 令 将 一 项 特 许 批 予 该 申 请 人 。         \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(train_x))\n",
    "    print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "    print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    initializer = tf.random_uniform_initializer(\n",
    "        -0.08, 0.08)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    x_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_real_len = tf.placeholder(\"int32\",[None])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "    \n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # encoder\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    cell_list = []\n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "    cell_list = []\n",
    "    \n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_backword_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_backword_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell,encoder_backword_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len,dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        encoder_state = tuple(encoder_state)\n",
    "    \n",
    "    # decoder \n",
    "    #decoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    cell_list = []\n",
    "    for i in range(layer_number):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        decoder_cell = cell_list[0]\n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    # Helper\n",
    "    \n",
    "    # attention\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        attention_hidden_size, encoder_outputs,\n",
    "        memory_sequence_length=x_real_len,scale=True)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=attention_output_size)\n",
    "    \n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        target_vocat_size, use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp,sequence_length= y_len)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "       \n",
    "        outputs, _,___  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_real_len, seq_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        helper_predict = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_decoder,\n",
    "            tf.fill([batch_size], ch2ind['<go>']), 0)\n",
    "        decoder_predict = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper_predict, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "        outputs_predict,_, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_predict, maximum_iterations=test_y.shape[1] * 2)\n",
    "    translations = outputs_predict.sample_id\n",
    "\n",
    "    # calculate loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        batch_size)\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(logits,target):\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target[:,:seq_max_len], logits[:,:seq_max_len]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#saver.restore(session,'middleresult/align/result_1_20847')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middleresult/result_deep'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'middleresult/result_deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middleresult/result_char'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saver.save(session,'middleresult/result_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import Dataset,ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "train_set = Dataset(train_x,train_y)\n",
    "test_set = Dataset(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bleu_score(predict,target):\n",
    "    try:\n",
    "        target = [[[j for index,j in enumerate(i) if j > 0 or index < 4]] for i in target]\n",
    "        predict = [[j for index,j in enumerate(i) if j > 0 or index < 4] for i in predict]\n",
    "        BLEUscore = nltk.translate.bleu_score.corpus_bleu(target,predict)\n",
    "    except:\n",
    "        BLEUscore = -1\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_test_loss(test_set = Dataset(test_x,test_y),display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(worksum):\n",
    "        batch_x,batch_y = test_set.next_batch(batch_size)\n",
    "        lx = [seq_max_len] * batch_size\n",
    "        ly = [seq_max_len] * batch_size\n",
    "        bx = [np.sum(m > 0) for m in batch_x]\n",
    "        by = [np.sum(m > 0) for m in batch_y]\n",
    "        tmp_loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,\n",
    "                                                     y_in:\n",
    "                                                     np.concatenate((\n",
    "                                                     np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                     ,x_len:lx,y_len:ly,\n",
    "                                                                        y_real_len:by,\n",
    "                                                                        x_real_len:bx})\n",
    "        loss_list.append(tmp_loss)\n",
    "        tmp_acc = cal_acc(tran,batch_y)\n",
    "        accs.append(tmp_acc)\n",
    "        predict_list += [i for i in tran]\n",
    "        target_list += [i for i in batch_y]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    return np.average(loss_list),np.average(accs),get_bleu_score(predict_list,target_list),predict_list,target_list,source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 300/300 \t used:190s eta:0 s"
     ]
    }
   ],
   "source": [
    "w_loss,w_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(Dataset(train_x[::100],train_y[::100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239.4696, 6.9010416666666669e-05, 0.005684131788259892)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_loss,w_acc,bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_text(x):\n",
    "    return [' '.join([ind2ch.get(j,'') for j in i]) for i in x]\n",
    "def get_all_en_text(x):\n",
    "    return [' '.join([ind2en.get(j,'') for j in i]) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['畑 畑 畑 畑 意 意 键 啜 啜 键 啜 啜 啜 啜 笾 笾 敤 想 炫 炫 炫 徼 徼 徼 徼 徼 徼 綫 綫 嵬 嵬 嵬 嵬 嵬 嵬 嵬 當 當 當 當 挽 當 骓 當 \\ue6dd 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 瑟 秤 秤 σ 由 秤 秤 旎 旎 秤 秤 秤 募 募 單 \\ue078 單 單 單 素 素',\n",
       " 'ｈ 畑 ｈ ｈ 炫 炫 炫 炫 炫 炫 炫 炫 狈 狈 狈 狈 狈 狈 狈 狈 狈 狈 狈 狈 們 鼽 赂 赂 赂 赂 赂 赂 赂 赂 赂 赂 赂 赂 赂 赂 赂 茕 赂 弥 弥 赂 赂 赂 赂 赂 齧 敘 镗 镗 镗 镗 镗 镗 镗 却 灭 镗 镗 ⒚ 镗 墟 墟 墟 吾 撒 撒 撒 線 吾 蹲 摞 哓 糜 哓 验',\n",
       " '迨 痷 畑 畑 禜 禜 禜 禜 禜 蕨 蕨 蕨 莨 檀 檀 莨 檀 蜇 蜇 蜇 绪 绪 $ 绪 绪 绪 绪 御 御 御 卖 御 御 御 劫 槲 槲 槲 劫 劫 劫 劫 劫 劫 劫 劫 劫 劫 劫 劫 劫 环 环 矛 溫 环 ㄚ 矛 淨 腊 淨 淨 伱 骑 め 骑 噁 骑 伱 伱 伱 伱 蛛 蛛 伱 罢 罢 罢 罢 怩',\n",
       " '颗 母 水 栭 \\ue367 \\ue367 釜 釜 逻 \\ue367 \\ue367 黛 滁 滁 黛 滁 挥 栭 挥 允 允 允 瑕 瑕 瑕 瑕 瑕 瑕 楤 瑕 瑕 瑕 瑕 瑕 瑕 瑕 瑕 仠 仠 仠 仠 \\ue710 \\ue710 嘎 瘸 嘎 嘎 嘎 嘎 缥 嘎 嘎 嘎 实 嘎 实 嘎 实 实 实 实 陪 栭 栭 栭 陪 苣 苣 苣 苣 苣 苣 為 僅 為 為 為 為 為 為',\n",
       " '可 可 可 可 萍 萍 萍 煤 煤 煤 煤 煤 煤 幡 幡 幡 幡 缱 缱 缱 幡 矛 警 笵 蜥 蜥 蜥 幼 幼 幼 幡 幼 幼 幼 幼 す 矬 枫 す 枫 枫 枫 枫 莸 莸 莸 枫 枫 枫 枫 枫 枫 枫 枫 枫 枫 枫 枫 枫 槲 沘 搡 搡 搡 搡 雜 煤 女 靺 誉 煺 煺 誉 煺 煺 ｉ 豁 豁 豁 惇',\n",
       " '畑 畑 畑 畑 畑 畑 畑 稿 炫 庐 庐 庐 庐 庐 雎 社 社 意 社 社 社 社 社 作 作 胸 胸 坫 坫 坫 作 當 當 當 剀 當 蜇 蜇 5 蜇 雎 5 5 5 5 5 \\ue60b \\ue60b \\ue60b \\ue60b 雎 \\ue60b \\ue60b 狞 \\ue60b 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 蜇 麓 蜇 蜇 蜇 屾 蜇 蜇 蜇 畑 畑 畑 牌 酤 酤 酤 酤',\n",
       " '籵 儆 儆 籵 籵 禜 禜 禜 禜 蕨 胲 社 胲 胲 胲 讳 讳 5 5 題 題 題 題 題 題 題 媞 媞 鮟 鮟 鮟 鮟 杈 \\ue7b2 \\ue7b2 \\ue7b2 \\ue7b2 \\ue7b2 \\ue7b2 劬 \\ue7b2 仅 城 城 城 界 城 界 \\ue7b2 栦 界 栦 栦 栦 界 栦 栦 栦 栦 叵 媞 馀 馀 馀 馀 馀 馀 馀 馀 馀 馀 臬 臬 臬 臬 槲 馀 進 镭 進',\n",
       " '畑 畑 畑 畑 畑 键 键 键 键 笾 键 笾 键 笾 笾 笾 砖 砖 砖 砖 欎 蜇 蜇 蜇 蜇 蜇 蜇 欎 蜇 蜇 蜇 蜇 蜇 蜇 欎 欎 欎 欎 欎 欎 墟 欎 墟 吖 陋 戝 陋 蹉 戝 鼽 兄 兄 蜇 蹉 蹉 蹉 蹉 蜇 蜇 梃 梃 梃 优 蜇 婊 婊 琮 琮 琮 當 當 當 當 當 當 鍎 蚴 鍎 綍 綍',\n",
       " '鼽 鼽 ｈ ｈ 漉 漉 霞 蓟 蓟 蓟 霞 霞 炫 那 那 匚 岛 岛 檀 檀 檀 檀 檀 檀 檀 檀 迨 迨 蓣 檀 檀 檀 侬 侬 檀 檀 檀 檀 檀 癫 癫 ｈ 檀 檀 迨 迨 艹 损 檀 损 镭 镭 俅 镭 镭 蕃 篷 蕃 蕃 蕃 蕃 蕃 蕃 唤 唤 唤 码 码 码 码 码 码 码 码 \\ue194 \\ue26d 嵇 \\ue26d \\ue26d 〈',\n",
       " '畑 畑 畑 畑 键 键 键 炫 炫 炫 炫 炫 炫 炫 炫 峁 峁 峁 峁 峁 峁 峁 峁 峁 峁 峁 峁 灯 哓 哓 哓 哓 ╧ 类 排 排 类 硅 排 楠 們 們 嫰 欎 嫰 辚 嫰 欎 欎 花 花 花 花 花 嘛 嘛 喂 钤 钤 钤 剝 剝 屌 欎 想 甬 想 甬 甬 檀 剀 剀 剀 剀 剀 剀 檀 檀 檀 檀']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_all_text(predict_list)\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['如 果 没 有 其 他 的 事 ， 在 他 的 回 忆 录 里 关 于 阿 尔 巴 尼 亚 的 译 文 会 使 其 成 为 一 本 畅 销 书 。   ',\n",
       " '我 们 要 设 法 削 减 本 年 度 的 开 支 。                          ',\n",
       " '( 3 ) 根 据 第 ( 1 ) 款 发 出 的 命 令 须 由 仲 裁 人 签 署 ， 并 须 送 交 各 当 事 人 及 交 书 记 存 档 。  ',\n",
       " '人 民 解 放 军 ， 在 戒 严 指 挥 机 构 的 统 一 部 署 下 ， 由 中 央 军 事 委 员 会 指 定 的 军 事 机 关 实 施 指 挥 。',\n",
       " '金 融 交 易 激 增 之 效 用 的 政 策 制 定 者 ， 这 种 激 增 除 了 为 银 行 家 带 来 丰 厚 报 酬 ， 再 无 其 它 效 用 。',\n",
       " '分 析 了 运 移 参 数 的 变 化 对 g c l 中 污 染 物 运 移 的 影 响 。                ',\n",
       " '溶 性 多 糖 （ w s p c ） 通 过 修 复 自 身 免 疫 损 伤 ， 促 进 胰 岛 素 分 泌 ， 有 显 著 的 降 低 血 糖 作 用 。',\n",
       " '在 某 种 程 度 上 还 标 识 着 我 们 对 于 宇 宙 的 的 物 理 认 知 的 崩 溃 。              ',\n",
       " '( 4 ) 在 本 部 适 用 的 任 何 船 舶 上 ， 不 得 在 限 界 线 之 下 的 船 壳 安 装 自 动 通 风 舷 窗 。     ',\n",
       " '1 9 7 0 年 左 右 ， 人 们 发 现 特 化 的 不 是 一 块 区 域                   ']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_all_text(target_list)\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('eval/tranwer.txt','w',encoding='utf-8') as whdl:\n",
    "    for line in texts:\n",
    "        whdl.write(\"{}\\n\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "#                                                                np.concatenate((\n",
    "#                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "#                                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tran.shape\n",
    "i_save = 0\n",
    "j_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(i_save,j_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'align_char_deep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('middleresult/{}'.format(model_path))\n",
    "os.mkdir('eval/{}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5 loss:51.32240676879883 lr:1 37.23 % [==================>--------------------------------] 11087/29780 \t used:98s eta:165 s"
     ]
    }
   ],
   "source": [
    "n_epoch = 60\n",
    "restore = True\n",
    "lr = 1\n",
    "for i in range(i_save,n_epoch):\n",
    "    \n",
    "    i_save = i\n",
    "    worksum = int(len(train_y)/batch_size)\n",
    "    pb = ProgressBar(worksum=worksum)\n",
    "    pb.startjob()\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for j in range(worksum):\n",
    "        if restore == True and j < j_save:\n",
    "            pb.finishsum += 1\n",
    "            continue\n",
    "        restore = False\n",
    "        \n",
    "        j_save = j\n",
    "        batch_x,batch_y = train_set.next_batch(batch_size)\n",
    "        lx = [seq_max_len] * batch_size\n",
    "        ly = [seq_max_len] * batch_size\n",
    "        bx = [np.sum(m > 0) for m in batch_x]\n",
    "        by = [np.sum(m > 0) for m in batch_y]\n",
    "        by =[m + 2  if m < seq_max_len - 1 else m for m in by ]\n",
    "        _, loss = session.run([optimizer,train_loss],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "                                                                np.concatenate((\n",
    "                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                                ,y_real_len:by,\n",
    "                                                                x_real_len:bx\n",
    "                                                               })\n",
    "        train_loss_list.append(loss)\n",
    "        #tmp_train_acc = cal_acc(tran,batch_y)\n",
    "        #train_acc_list.append(tmp_train_acc)\n",
    "        pb.info = \"iter {} loss:{} lr:{}\".format(i + 1,loss,lr)\n",
    "        val_step = int(worksum / 4)\n",
    "        if j % val_step == 0 and j != 0:\n",
    "            test_loss,test_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss()\n",
    "            _,train_acc,train_bleu_score,train_predict_list,train_target_list,train_source_list = calc_test_loss(Dataset(train_x[::100],train_y[::100]),display=False)\n",
    "            predict_texts = get_all_text(predict_list)\n",
    "            target_texts = get_all_text(target_list)\n",
    "            source_texts = get_all_en_text(source_list)\n",
    "            \n",
    "            train_predict_texts = get_all_text(train_predict_list)\n",
    "            train_target_texts = get_all_text(train_target_list)\n",
    "            train_source_texts = get_all_en_text(train_source_list)\n",
    "            \n",
    "            with open('eval/{}/{}_{}_predict'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "                    \n",
    "            with open('eval/{}/{}_{}_predict_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            print(\"\\niter {} step {} train loss {} train acc {} test loss {} test acc {} bleu {} lr {}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            try:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session,'middleresult/{}/result_{}_{}'.format(model_path,i + 1,j))\n",
    "            except:\n",
    "                print('save fail')\n",
    "        lr_step = int(worksum / 2) - 1\n",
    "        if j % lr_step == 0 and j != 0:\n",
    "            if (i + 1) >= 6:\n",
    "                lr = lr / 2\n",
    "        pb.complete(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x13864e0aa90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ind2en.get(i,'') for i in test_x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = 'do you know where is the nearest shopping center ?'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'you',\n",
       " 'know',\n",
       " 'where',\n",
       " 'is',\n",
       " 'the',\n",
       " 'nearest',\n",
       " 'shopping',\n",
       " 'center',\n",
       " '?']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = [en2ind.get(i) for i in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = tf.contrib.keras.preprocessing.sequence.pad_sequences([sents],seq_max_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  62,   16,  101,  129,   12,    2, 6984, 2924,  858,   39,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 62,  16, 101, ...,   0,   0,   0],\n",
       "       [ 62,  16, 101, ...,   0,   0,   0],\n",
       "       [ 62,  16, 101, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [ 62,  16, 101, ...,   0,   0,   0],\n",
       "       [ 62,  16, 101, ...,   0,   0,   0],\n",
       "       [ 62,  16, 101, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(sents,35,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sents[0] > 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tran = session.run([translations],feed_dict={x:np.repeat(sents,64,axis=0),x_len:[35] * 64, x_real_len:[sum(sents[0] > 0) + 1] * 64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你', '知道', '最近', '的', '购物中心', '哪里', '吗', '？', '', '', '']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ind2ch.get(i,'') for i in tran[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# release the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('release/align_and_translate_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'release/align_and_translate/align_and_translate_model'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'release/align_and_translate_char/align_and_translate_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
