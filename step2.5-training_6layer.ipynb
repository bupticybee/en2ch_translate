{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import tflearn\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('middleresult/en_ch_35word.pkl','wb') as whdl:\n",
    "    pickle.dump((\n",
    "        train_x,\n",
    "        test_x,\n",
    "        train_y,\n",
    "        test_y,\n",
    "        ind2ch,\n",
    "        ch2ind,\n",
    "        ind2en,\n",
    "        en2ind,\n",
    "    ),whdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/preprocessing_tokenlizer/sentence_tokened_by_word.pkl','rb') as fhdl:\n",
    "    (\n",
    "         ind2ch,\n",
    "         ch2ind,\n",
    "         ind2en,\n",
    "         en2ind,\n",
    "         train_x,\n",
    "         train_y,\n",
    "    ) = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/preprocessing_subword/subwords_allwords.en','rb') as fhdl:\n",
    "    en_subword = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_subword_dic = dict(zip(en_subword['origin'],en_subword['segmented']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('middleresult/char/zh_vocab.txt',encoding='utf-8') as fhdl:\n",
    "    ch_subwords = [line.strip().split(\"\\t\") for line in fhdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661850, 406740)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind2ch),len(ind2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_inv_size_base = 40000#len(ind2en) + 3\n",
    "target_inv_size_base = 40000#len(ind2ch) + 3\n",
    "\n",
    "USE_GPU = 1\n",
    "\n",
    "attention_hidden_size = 1024\n",
    "attention_output_size = 1024\n",
    "embedding_size = 1024\n",
    "seq_max_len = 60\n",
    "num_units = 1024\n",
    "batch_size = 64\n",
    "layer_number = 6\n",
    "max_grad = 1.0\n",
    "dropout = 0.2\n",
    "sentence_max_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_inv = list(map(lambda x:x[0],sorted(ch2ind.items(),key=lambda x:x[1])[:target_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_inv = list(map(lambda x:x[0],sorted(en2ind.items(),key=lambda x:x[1])[:src_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_inv = en_inv[:3] + ['_' + i for i in en_inv[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_inv_tmpdic = dict(zip(ch_inv,range(len(ch_inv))))\n",
    "ch_oov = [i[0] for i in ch_subwords if i[0] not in ch_inv_tmpdic]\n",
    "en_inv_tmpdic = dict(zip(en_inv,range(len(en_inv))))\n",
    "en_oov = [i for i in en_subword['subwords'] if i not in en_inv_tmpdic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2ch_oov = dict(zip(range(len(ch_oov) + len(ch_inv)),ch_inv + ch_oov))\n",
    "ch2ind_oov = dict(zip(ch_inv + ch_oov,range(len(ch_oov) + len(ch_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2en_oov = dict(zip(range(len(en_oov) + len(en_inv)),en_inv + en_oov))\n",
    "en2ind_oov = dict(zip(en_inv + en_oov,range(len(en_oov) + len(en_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5846, 3466)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_oov),len(en_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = src_inv_size_base + len(en_oov)\n",
    "target_vocat_size = target_inv_size_base + len(ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43466, 43466, 45846, 45846)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size,len(ind2en_oov),target_vocat_size,len(ind2ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 10000000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360, 2360)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ind['james'],en2ind_oov['_james']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = [i[::-1] for i in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = sequence.pad_sequences(train_x,seq_max_len,padding='post',value=en2ind['<eos>'])\n",
    "#train_y = sequence.pad_sequences(train_y,seq_max_len,padding='post',value=ch2ind['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go , elvis !\n",
      "上 ， 爱 维斯 ！\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0,len(train_x))\n",
    "print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_x,train_y , test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def en_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < src_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            en_word = ind2en[wordind]\n",
    "            if en_word not in en_subword_dic:\n",
    "                sentence_inds.append(en2ind_oov['<unk>'])\n",
    "            en_pieces = en_subword_dic[en_word]\n",
    "            pieces_index = [en2ind_oov[i] for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ch_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < target_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            ch_word = ind2ch[wordind]\n",
    "            en_pieces = [i for i in ch_word]\n",
    "            pieces_index = [ch2ind_oov.get(i,ch2ind_oov['<unk>']) for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tmp = []\n",
    "for index,sentence in enumerate(train_x):\n",
    "    train_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_tmp = []\n",
    "for index,sentence in enumerate(train_y):\n",
    "    train_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_tmp = []\n",
    "for index,sentence in enumerate(test_x):\n",
    "    test_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_tmp = []\n",
    "for index,sentence in enumerate(test_y):\n",
    "    test_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x_tmp)\n",
    "train_y = np.asarray(train_y_tmp)\n",
    "test_x = np.asarray(test_x_tmp)\n",
    "test_y = np.asarray(test_y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x_tmp\n",
    "del train_y_tmp\n",
    "del test_x_tmp\n",
    "del test_y_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_so _if _i _want _daniel _to _do _something _,\n",
      "所以 我 想 让 丹尼尔 做 什么 ，\n",
      "_i _can _get _you _back _- _thank _you _, _but _no _thank _you _.\n",
      "我 可以 带 你 回去 - 谢 了 ， 不必 麻烦 了 。\n",
      "_you _know _, _we _could _take _one _down _, _cut _the _nylon _into _strips _.\n",
      "你 知道 我们 可以 拆 一个 下来 把 那些 尼龙 弄 成 条 。\n",
      "_follow _me _. _come _along _.\n",
      "你们 跟 我 上去 。 过来 。\n",
      "_great _. _i _' _m _gonna _grab _some _lunch _.\n",
      "好 。 我 想 去 吃 点 午饭 。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(train_x))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in train_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:{}'.format(USE_GPU)):\n",
    "    #initializer = tf.random_uniform_initializer(\n",
    "    #    -0.08, 0.08)\n",
    "    initializer = tf.truncated_normal_initializer(\n",
    "        mean=0.0,stddev=0.02)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    x_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_max_len = tf.placeholder(tf.int32, shape=[])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "    \n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # encoder\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    cell_list = []\n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "    cell_list = []\n",
    "    \n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_backword_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_backword_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell,encoder_backword_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len,dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        encoder_state = tuple(encoder_state)\n",
    "    \n",
    "    # decoder \n",
    "    #decoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    cell_list = []\n",
    "    for i in range(layer_number):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        decoder_cell = cell_list[0]\n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    # Helper\n",
    "    \n",
    "    # attention\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        attention_hidden_size, encoder_outputs,\n",
    "        memory_sequence_length=x_real_len,scale=True)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=attention_output_size)\n",
    "    \n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        target_vocat_size, use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp,sequence_length= y_len)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "       \n",
    "        outputs, _,___  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_real_len, y_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        helper_predict = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_decoder,\n",
    "            tf.fill([batch_size], ch2ind['<go>']), ch2ind['<eos>'])\n",
    "        decoder_predict = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper_predict, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "        outputs_predict,_, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_predict, maximum_iterations=sentence_max_length)\n",
    "    translations = outputs_predict.sample_id\n",
    "\n",
    "    # calculate loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        batch_size)\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(logits,target):\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target[:,:seq_max_len], logits[:,:seq_max_len]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver.restore(session,'middleresult/align/result_1_20847')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middleresult/result_deep'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'middleresult/result_deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saver.save(session,'middleresult/result_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import Dataset,ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bleu_score(predict,target):\n",
    "    try:\n",
    "        target = [[[j for index,j in enumerate(i)]] for i in target]\n",
    "        predict = [[j for index,j in enumerate(i)] for i in predict]\n",
    "        BLEUscore = nltk.translate.bleu_score.corpus_bleu(target,predict)\n",
    "    except:\n",
    "        BLEUscore = -1\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x),len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_len = [len(i) for i in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = list(filter(lambda x:x[2] < 50,sorted(zip(test_x,test_y,test_x_len),key=lambda x:x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.asarray([i[0] for i in tmp])\n",
    "test_y = np.asarray([i[1] for i in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_happens _sometimes _.\n",
      "有时候 就 这样 。\n",
      "_print _that _one _.\n",
      "把 这 帧 打印 出来 。\n",
      "_except _, _nowadays _,\n",
      "只不过 ， 现如今 ，\n",
      "_sure _we _will _.\n",
      "会 忙 完 的 。\n",
      "_the _college _sued _?\n",
      "学院 已经 被 起诉 了 ？\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(test_x[:1500]))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in test_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in test_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99422, 99422)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_test_loss(test_x,test_y,display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(0,len(test_x),batch_size):\n",
    "        batch_x,batch_y = test_x[j:j + batch_size],test_y[j:j + batch_size]#test_set.next_batch(batch_size)\n",
    "        if len(batch_x) < batch_size:\n",
    "            continue\n",
    "        bx = [len(m) + 1 for m in batch_x]\n",
    "        by = [len(m) + 1 for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        \n",
    "        tmp_loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,\n",
    "                                                     y_in:\n",
    "                                                     np.concatenate((\n",
    "                                                     np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                     ,x_len:lx,y_len:ly,\n",
    "                                                                        y_real_len:by,\n",
    "                                                                        x_real_len:bx,\n",
    "                                                                        y_max_len:max(by)\n",
    "                                                                        })\n",
    "        loss_list.append(tmp_loss)\n",
    "        tmp_acc = cal_acc(tran,batch_y)\n",
    "        accs.append(tmp_acc)\n",
    "        predict_list += [i for i in tran]\n",
    "        target_list += [i for i in batch_y]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    return np.average(loss_list),np.average(accs),get_bleu_score(predict_list,target_list),predict_list,target_list,source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 15/15 \t used:18s eta:0 s"
     ]
    }
   ],
   "source": [
    "w_loss,w_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(train_x[::10000],train_y[::10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.782 0.0 0.06210944975647665\n"
     ]
    }
   ],
   "source": [
    "print(w_loss,w_acc,bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_text(x):\n",
    "    return [' '.join([ind2ch_oov.get(j,'') for j in i]) for i in x]\n",
    "def get_all_en_text(x):\n",
    "    return [' '.join([ind2en_oov.get(j,'') for j in i]) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "#                                                                np.concatenate((\n",
    "#                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "#                                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tran.shape\n",
    "i_save = 0\n",
    "j_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(i_save,j_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'OOVSUB_6layer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('middleresult/{}'.format(model_path)):os.mkdir('middleresult/{}'.format(model_path))\n",
    "if not os.path.exists('eval/{}'.format(model_path)):os.mkdir('eval/{}'.format(model_path))\n",
    "if not os.path.exists('val/{}'.format(model_path)):os.mkdir('val/{}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tmpindexs(train_index_set):\n",
    "    tmp_indexs,_ = train_index_set.next_batch(batch_size * batch_gen)\n",
    "    tmp_length = [len(train_x[i]) for i in tmp_indexs]\n",
    "    tmp_indexs = [i[0] for i in sorted(zip(tmp_indexs,tmp_length),key=lambda x:x[1])]\n",
    "    tmp = []\n",
    "    for i in random.sample(range(batch_gen),batch_gen):\n",
    "        tmp += tmp_indexs[i * batch_size:(i + 1) * batch_size]\n",
    "    tmp_indexs = tmp\n",
    "    return tmp_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_indexs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 154/154 \t used:149s eta:0 s54687 \t used:1350s eta:4050 ss\n",
      "iter 1 step 38671 train loss 52.969268798828125 train acc 0.28526396939216375 test loss 50.86732864379883 test acc 0.22739587050543333 bleu 0.07144546059861599 lr 1\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:142s eta:0 s54687 \t used:29134s eta:29135 ss\n",
      "iter 1 step 77342 train loss 48.62263870239258 train acc 0.30561490181540374 test loss 44.269752502441406 test acc 0.23792647625047295 bleu 0.08355874892194728 lr 1\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:140s eta:0 s154687 \t used:54482s eta:18163 ss\n",
      "iter 1 step 116013 train loss 43.9130859375 train acc 0.30366145568090835 test loss 41.12855529785156 test acc 0.25253399452022945 bleu 0.09958403310650299 lr 1\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:132s eta:0 s154687 \t used:81971s eta:5 ssssss\n",
      "iter 1 step 154684 train loss 41.10395431518555 train acc 0.3280710412049073 test loss 39.02515411376953 test acc 0.27012306618499043 bleu 0.1184770232325007 lr 1\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:135s eta:0 s4687 \t used:27239s eta:81733 ssss\n",
      "iter 2 step 38671 train loss 38.964683532714844 train acc 0.2947024265738217 test loss 37.42042922973633 test acc 0.25643414228233763 bleu 0.10465320379677027 lr 0.5\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:139s eta:0 s30/154687 \t used:58680s eta:58698 s\n",
      "iter 2 step 77342 train loss 36.808536529541016 train acc 0.3034350567614539 test loss 35.557315826416016 test acc 0.2695543522608553 bleu 0.11711030262929178 lr 0.25\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:138s eta:0 s5999/154687 \t used:86537s eta:28860 s\n",
      "iter 2 step 116013 train loss 35.68044662475586 train acc 0.31102360575258786 test loss 34.688087463378906 test acc 0.2675970113495658 bleu 0.11690471036731595 lr 0.125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:133s eta:0 s4666/154687 \t used:118093s eta:14 ssss\n",
      "iter 2 step 154684 train loss 35.06404113769531 train acc 0.306134564572857 test loss 34.23158264160156 test acc 0.2716905044580919 bleu 0.12879387612003984 lr 0.0625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:133s eta:0 s666/154687 \t used:31215s eta:93660 sss\n",
      "iter 3 step 38671 train loss 34.282962799072266 train acc 0.30675620916151125 test loss 34.03456115722656 test acc 0.27181053692216894 bleu 0.12825332184419985 lr 0.03125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:132s eta:0 s77334/154687 \t used:58749s eta:58761 s\n",
      "iter 3 step 77342 train loss 34.20903778076172 train acc 0.30424029818017534 test loss 33.933502197265625 test acc 0.27006177034048245 bleu 0.12659828772000978 lr 0.015625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:534s eta:0 ss 116003/154687 \t used:86885s eta:28972 s\n",
      "iter 3 step 116013 train loss 34.163509368896484 train acc 0.30247272753007487 test loss 33.876834869384766 test acc 0.27504193273901756 bleu 0.13103648344343452 lr 0.0078125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 154/154 \t used:591s eta:0 ss] 154674/154687 \t used:173812s eta:12 sss\n",
      "iter 3 step 154684 train loss 34.13113784790039 train acc 0.30870093812659033 test loss 33.86086654663086 test acc 0.2668868515843999 bleu 0.12747038654828305 lr 0.00390625\n",
      "\n",
      "iter 4 loss:33.32698893947073 lr:0.00390625 1.40 % [>--------------------------------------------------] 2173/154687 \t used:7466s eta:523670 ss"
     ]
    }
   ],
   "source": [
    "n_epoch = 60\n",
    "restore = True\n",
    "lr = 1\n",
    "\n",
    "batch_gen = 100\n",
    "\n",
    "from utils import *\n",
    "if not restore:\n",
    "    train_index_set = Dataset(np.arange(len(train_x)),np.arange(len(train_y)))\n",
    "    tmp_indexs = []\n",
    "    \n",
    "exp_loss = None\n",
    "alpha = 0.97\n",
    "for i in range(i_save,n_epoch):\n",
    "    one_epoch = i + 1\n",
    "    i_save = i\n",
    "    worksum = int(len(train_y)/batch_size)\n",
    "    pb = ProgressBar(worksum=worksum)\n",
    "    pb.startjob()\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for j in range(worksum):\n",
    "        one_batch = j\n",
    "        if restore == True and j < j_save:\n",
    "            pb.finishsum += 1\n",
    "            continue\n",
    "        restore = False\n",
    "        \n",
    "        j_save = j\n",
    "        \n",
    "        if tmp_indexs == []:\n",
    "            tmp_indexs = get_tmpindexs(train_index_set)\n",
    "        batch_indexs,tmp_indexs = tmp_indexs[:batch_size],tmp_indexs[batch_size:]\n",
    "        batch_x,batch_y = train_x[batch_indexs],train_y[batch_indexs]\n",
    "\n",
    "        bx = [min(len(m) + 1,seq_max_len) for m in batch_x]\n",
    "        by = [min(len(m) + 1,seq_max_len) for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        #print(batch_x.shape,batch_y.shape)\n",
    "        try:\n",
    "            _, loss = session.run([optimizer,train_loss],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "                                                                    np.concatenate((\n",
    "                                                                    np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                                    ,y_real_len:by,\n",
    "                                                                    x_real_len:bx,\n",
    "                                                                    y_max_len:max(by)\n",
    "                                                                   })\n",
    "        except:\n",
    "            with open(\"log.txt\",'a') as whdl:\n",
    "                whdl.write(\"iter {} batch {} error\\n\".format(i + 1 ,j))\n",
    "            continue\n",
    "        train_loss_list.append(loss)\n",
    "        #tmp_train_acc = cal_acc(tran,batch_y)\n",
    "        #train_acc_list.append(tmp_train_acc)\n",
    "        exp_loss = loss if exp_loss == None else alpha * exp_loss + (1 - alpha) * loss\n",
    "        pb.info = \"iter {} loss:{} lr:{}\".format(i + 1,exp_loss,lr)\n",
    "        with open('val/{}/train_loss.txt'.format(model_path),'a') as whdl:\n",
    "            whdl.write(\"{}\\t{}\\t{}\\n\".format(one_epoch,one_batch,loss))\n",
    "        val_step = int(worksum / 4)\n",
    "        if j % val_step == 0 and j != 0:\n",
    "            test_loss,test_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(test_x[::4],test_y[::4])\n",
    "            _,train_acc,train_bleu_score,train_predict_list,train_target_list,train_source_list = calc_test_loss(train_x[::1000],train_y[::1000])\n",
    "            predict_texts = get_all_text(predict_list)\n",
    "            target_texts = get_all_text(target_list)\n",
    "            source_texts = get_all_en_text(source_list)\n",
    "            \n",
    "            train_predict_texts = get_all_text(train_predict_list)\n",
    "            train_target_texts = get_all_text(train_target_list)\n",
    "            train_source_texts = get_all_en_text(train_source_list)\n",
    "            \n",
    "            with open('eval/{}/{}_{}_predict'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "                    \n",
    "            with open('eval/{}/{}_{}_predict_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            print(\"\\niter {} step {} train loss {} train acc {} test loss {} test acc {} bleu {} lr {}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            with open('val/{}/test_loss.txt'.format(model_path),'a') as whdl:\n",
    "                whdl.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            try:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session,'middleresult/{}/result_{}_{}'.format(model_path,i + 1,j))\n",
    "            except:\n",
    "                print('save fail')\n",
    "        lr_step = int(worksum / 4) - 1\n",
    "        if j % lr_step == 0 and j != 0:\n",
    "            if (i + 1) >= 2:\n",
    "                lr = lr / 2\n",
    "        pb.complete(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
