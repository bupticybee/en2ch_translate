{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import ProgressBar,Dataset\n",
    "import pickle\n",
    "import random\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.00 % [==================================================>] 10000000/10000000 \t used:82s eta:0 s"
     ]
    }
   ],
   "source": [
    "en_sentences = []\n",
    "zh_sentences = []\n",
    "\n",
    "en_sentences_len = []\n",
    "zh_sentences_len = []\n",
    "\n",
    "pb = ProgressBar(worksum=10000000)\n",
    "pb.startjob()\n",
    "with open('middleresult/segmented_train_seg_by_word.txt') as fhdl:\n",
    "    english_flag = False\n",
    "    num = 0\n",
    "    for line in fhdl:\n",
    "        num += 1 if english_flag else 0\n",
    "        linesp = line.strip()\n",
    "        linesp = linesp.split(' ')\n",
    "        english_flag = not english_flag \n",
    "        if english_flag:\n",
    "            en_sentences.append(linesp)\n",
    "            en_sentences_len.append(len(linesp))\n",
    "        else:\n",
    "            zh_sentences.append(linesp)\n",
    "            zh_sentences_len.append(len(linesp))\n",
    "        if english_flag and num % 10000 == 0:\n",
    "            pb.complete(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_words = list(map(lambda x:x.split('\\t'), open('middleresult/en_vocab.txt').read().split('\\n')))[:-1]\n",
    "zh_words = list(map(lambda x:x.split('\\t'), open('middleresult/zh_vocab.txt').read().split('\\n')))[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 50\n",
    "english_words = 130000\n",
    "chinese_words = 120000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENLIZER BY WORD\n",
    "# OOV strategy : throw away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialdic = {0:'<unk>',1:'<eos>',2:'<go>'}\n",
    "\n",
    "en2index,index2en = {},{}\n",
    "zh2index,index2zh = {},{}\n",
    "\n",
    "for index,word in specialdic.items():\n",
    "    en2index[word] = index\n",
    "    zh2index[word] = index\n",
    "    index2en[index] = word\n",
    "    index2zh[index] = word\n",
    "    \n",
    "for index,ext in enumerate(en_words):\n",
    "    (word,times) = ext\n",
    "    index += len(specialdic)\n",
    "    en2index[word] = index\n",
    "    index2en[index] = word\n",
    "\n",
    "for index,ext in enumerate(zh_words):\n",
    "    (word,times) = ext\n",
    "    index += len(specialdic)\n",
    "    zh2index[word] = index\n",
    "    index2zh[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2index['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing english sentences 100.00 % [==================================================>] 10000000/10000000 \t used:74s eta:0 s\n",
      "processing chinese sentences 100.00 % [==================================================>] 10000000/10000000 \t used:62s eta:0 s"
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "pb = ProgressBar(worksum=len(en_sentences),info=\"processing english sentences\")\n",
    "pb.startjob()\n",
    "for index,sentence in enumerate(en_sentences):\n",
    "    train_x.append([en2index.get(word,en2index['<unk>']) for word in sentence])\n",
    "    if index % 1000 == 0:\n",
    "        pb.complete(1000)\n",
    "    \n",
    "print()\n",
    "    \n",
    "train_y = []\n",
    "pb = ProgressBar(worksum=len(zh_sentences),info=\"processing chinese sentences\")\n",
    "pb.startjob()\n",
    "for index,sentence in enumerate(zh_sentences):\n",
    "    train_y.append([zh2index.get(word,zh2index['<unk>']) for word in sentence])\n",
    "    if index % 1000 == 0:\n",
    "        pb.complete(1000)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the education for higher vocational students majoring in engineering supervision mainly revolves around cultivating and improving \" vocational ablity \" .\n",
      "高职 工程 监理 专业 学生 的 教育 主要 是 围绕 培养 和 提高 “ 职业 能力 ” 来 进行 。\n",
      "\n",
      "okay , okay , i get it . what ?\n",
      "好 吧 ， 是 的 ， 我 懂 了 。 什么 ？\n",
      "\n",
      "well , that i have spent a great deal of time on .\n",
      "不 我 花 了 很多 时间 的 那个 。\n",
      "\n",
      "standard classification for serviceability of an office facility for manageability .\n",
      "可管理性 的 办公设备 的 适用性 标准 分类 。\n",
      "\n",
      "i ' m a button pusher , spam in a can .\n",
      "我 就 会 按些 按钮 像 个 罐装 的 牛肉 而已 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for round in range(5):\n",
    "    index = random.choice(range(len(en_sentences)))\n",
    "    print(' '.join([index2en.get(i) for i in train_x[index]]))\n",
    "    print(' '.join([index2zh.get(i) for i in train_y[index]]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir data/preprocessing_tokenlizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/preprocessing_tokenlizer/sentence_tokened_by_word.pkl','wb') as whdl:\n",
    "    pickle.dump((\n",
    "         index2zh,\n",
    "         zh2index,\n",
    "         index2en,\n",
    "         en2index,\n",
    "         train_x,\n",
    "         train_y,\n",
    "    ),whdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 mtq mtq 755128925 Sep  8 17:38 data/preprocessing_tokenlizer/sentence_tokened_by_word.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -l data/preprocessing_tokenlizer/sentence_tokened_by_word.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
