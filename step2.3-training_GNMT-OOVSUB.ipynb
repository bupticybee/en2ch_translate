{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.keras import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('middleresult/en_ch_35word.pkl','wb') as whdl:\n",
    "    pickle.dump((\n",
    "        train_x,\n",
    "        test_x,\n",
    "        train_y,\n",
    "        test_y,\n",
    "        ind2ch,\n",
    "        ch2ind,\n",
    "        ind2en,\n",
    "        en2ind,\n",
    "    ),whdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problems we try to solve here:\n",
    "1. bleu calculate method\n",
    "2. GNMT cell \n",
    "3. add  model complex\n",
    "4. beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/preprocessing_tokenlizer/sentence_tokened_by_word.pkl','rb') as fhdl:\n",
    "    (\n",
    "         ind2ch,\n",
    "         ch2ind,\n",
    "         ind2en,\n",
    "         en2ind,\n",
    "         train_x,\n",
    "         train_y,\n",
    "    ) = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/preprocessing_subword/subwords_allwords.en','rb') as fhdl:\n",
    "    en_subword = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_subword_dic = dict(zip(en_subword['origin'],en_subword['segmented']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('middleresult/char/zh_vocab.txt',encoding='utf-8') as fhdl:\n",
    "    ch_subwords = [line.strip().split(\"\\t\") for line in fhdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661850, 406740)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind2ch),len(ind2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_inv_size_base = 40000#len(ind2en) + 3\n",
    "target_inv_size_base = 40000#len(ind2ch) + 3\n",
    "\n",
    "USE_GPU = 1\n",
    "\n",
    "attention_hidden_size = 1024\n",
    "attention_output_size = 1024\n",
    "embedding_size = 1024\n",
    "seq_max_len = 60\n",
    "num_units = 1024\n",
    "batch_size = 64\n",
    "layer_number = 4\n",
    "max_grad = 1.0\n",
    "dropout = 0.2\n",
    "sentence_max_length = 70\n",
    "beam_width = 3\n",
    "length_penalty_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_inv = list(map(lambda x:x[0],sorted(ch2ind.items(),key=lambda x:x[1])[:target_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_inv = list(map(lambda x:x[0],sorted(en2ind.items(),key=lambda x:x[1])[:src_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_inv = en_inv[:3] + ['_' + i for i in en_inv[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_inv_tmpdic = dict(zip(ch_inv,range(len(ch_inv))))\n",
    "ch_oov = [i[0] for i in ch_subwords if i[0] not in ch_inv_tmpdic]\n",
    "en_inv_tmpdic = dict(zip(en_inv,range(len(en_inv))))\n",
    "en_oov = [i for i in en_subword['subwords'] if i not in en_inv_tmpdic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2ch_oov = dict(zip(range(len(ch_oov) + len(ch_inv)),ch_inv + ch_oov))\n",
    "ch2ind_oov = dict(zip(ch_inv + ch_oov,range(len(ch_oov) + len(ch_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2en_oov = dict(zip(range(len(en_oov) + len(en_inv)),en_inv + en_oov))\n",
    "en2ind_oov = dict(zip(en_inv + en_oov,range(len(en_oov) + len(en_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5846, 3466)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_oov),len(en_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = src_inv_size_base + len(en_oov)\n",
    "target_vocat_size = target_inv_size_base + len(ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43466, 43466, 45846, 45846)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size,len(ind2en_oov),target_vocat_size,len(ind2ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 10000000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360, 2360)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ind['james'],en2ind_oov['_james']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = [i[::-1] for i in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = sequence.pad_sequences(train_x,seq_max_len,padding='post',value=en2ind['<eos>'])\n",
    "#train_y = sequence.pad_sequences(train_y,seq_max_len,padding='post',value=ch2ind['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you see these hands ? they ' re digging machines .\n",
      "你 看到 我 的 手 了 ？ 这 双手 就是 掘土机 。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0,len(train_x))\n",
    "print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_x,train_y , test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def en_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < src_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            en_word = ind2en[wordind]\n",
    "            if en_word not in en_subword_dic:\n",
    "                sentence_inds.append(en2ind_oov['<unk>'])\n",
    "            en_pieces = en_subword_dic[en_word]\n",
    "            pieces_index = [en2ind_oov[i] for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ch_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < target_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            ch_word = ind2ch[wordind]\n",
    "            en_pieces = [i for i in ch_word]\n",
    "            pieces_index = [ch2ind_oov.get(i,ch2ind_oov['<unk>']) for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tmp = []\n",
    "for index,sentence in enumerate(train_x):\n",
    "    train_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_tmp = []\n",
    "for index,sentence in enumerate(train_y):\n",
    "    train_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_tmp = []\n",
    "for index,sentence in enumerate(test_x):\n",
    "    test_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_tmp = []\n",
    "for index,sentence in enumerate(test_y):\n",
    "    test_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x_tmp)\n",
    "train_y = np.asarray(train_y_tmp)\n",
    "test_x = np.asarray(test_x_tmp)\n",
    "test_y = np.asarray(test_y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x_tmp\n",
    "del train_y_tmp\n",
    "del test_x_tmp\n",
    "del test_y_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_how _will _we _take _advantage _of _the _opportunities _to _waste _cycles _that _we _' _ll _get _from _new _, _faster _hardware _?\n",
      "我们 如何 从 浪费 更新 、 更快 的 硬件 的 机 时 中 获取 好处 呢 ？\n",
      "_that _' _s _all _. _cards _. _only _cards _from _now _on _.\n",
      "就 这样 了 。 卡片 。 现在 开始 只有 卡片\n",
      "_i _see _you _don _' _t _keep _track _of _your _kil is _.\n",
      "你们 不 统计 击落 的 敌机 数量 。\n",
      "_nature _is _a _principle _, _is _the _fundamental _principles _of _.\n",
      "返 朴 归 真 是 原则 ， 道 法 自然 是 根本 。\n",
      "_but _because _borders _is _facing _even _bigger _challenges _than _barnes _& _noble _, _it _may _not _be _seen _favorably _as _an _acquirer _.\n",
      "但 由于 b o r d e r s 面临 着 比 b a r n e s & n o b l e 更大 的 挑战 ， 它 似乎 不太 适合 作 收 购 方 。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(train_x))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in train_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:{}'.format(USE_GPU)):\n",
    "    #initializer = tf.random_uniform_initializer(\n",
    "    #    -0.08, 0.08)\n",
    "    initializer = tf.truncated_normal_initializer(\n",
    "        mean=0.0,stddev=0.02)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    x_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_max_len = tf.placeholder(tf.int32, shape=[])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "    \n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # encoder\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    cell_list = []\n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "    cell_list = []\n",
    "    \n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_backword_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_backword_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell,encoder_backword_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len,dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        encoder_state = tuple(encoder_state)\n",
    "    \n",
    "    # decoder \n",
    "    #decoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    cell_list = []\n",
    "    for i in range(layer_number):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        decoder_cell = cell_list[0]\n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    # Helper\n",
    "    \n",
    "    # attention\n",
    "    \n",
    "    \n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        target_vocat_size, use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "            attention_hidden_size, encoder_outputs,\n",
    "            memory_sequence_length=x_real_len,scale=True)\n",
    "        decoder_cell_att = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cell, attention_mechanism,\n",
    "            attention_layer_size=attention_output_size)\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp,sequence_length= y_len)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell_att, helper, initial_state = decoder_cell_att.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "       \n",
    "        outputs, _,___  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_real_len, y_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        #helper_predict = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "        #    embedding_decoder,\n",
    "        #    tf.fill([batch_size], ch2ind['<go>']), ch2ind['<eos>'])\n",
    "        #decoder_predict = tf.contrib.seq2seq.BasicDecoder(\n",
    "        #    decoder_cell, helper_predict, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "        #    output_layer=projection_layer)\n",
    "        \n",
    "        \n",
    "        #    tf.contrib.seq2seq.tile_batch(decoder_start, multiplier=beam_width)\n",
    "        #tf.contrib.seq2seq.tile_batch(\n",
    "        #    decoder_start, multiplier=beam_width)\n",
    "        encoder_outputs_infer = tf.contrib.seq2seq.tile_batch(\n",
    "          encoder_outputs, multiplier=beam_width)\n",
    "        x_real_len_infer = tf.contrib.seq2seq.tile_batch(\n",
    "          x_real_len, multiplier=beam_width)\n",
    "        \n",
    "        \n",
    "        \n",
    "        attention_mechanism_infer = tf.contrib.seq2seq.LuongAttention(\n",
    "            attention_hidden_size, encoder_outputs_infer,\n",
    "            memory_sequence_length=x_real_len_infer,scale=True)\n",
    "        decoder_cell_infer = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            decoder_cell, attention_mechanism_infer,\n",
    "            attention_layer_size=attention_output_size)\n",
    "        \n",
    "        decoder_start = decoder_cell_infer.zero_state(dtype=tf.float32,batch_size=batch_size * beam_width)\n",
    "        decoder_initial_state = decoder_start\n",
    "        \n",
    "        decoder_predict = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "              cell=decoder_cell_infer,\n",
    "              embedding=embedding_decoder,\n",
    "              start_tokens=tf.fill([batch_size], ch2ind_oov['<go>']),\n",
    "              end_token=ch2ind_oov['<eos>'],\n",
    "              initial_state=decoder_initial_state,\n",
    "              beam_width=beam_width,\n",
    "              output_layer=projection_layer,\n",
    "              length_penalty_weight=length_penalty_weight)\n",
    "        \n",
    "        outputs_predict,_, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_predict, maximum_iterations=sentence_max_length)\n",
    "    translations = outputs_predict.predicted_ids\n",
    "\n",
    "    # calculate loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        batch_size)\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(logits,target):\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target[:,:seq_max_len], logits[:,:seq_max_len]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saver.restore(session,'middleresult/align/result_1_20847')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middleresult/result_deep'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'middleresult/result_deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saver.save(session,'middleresult/result_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import Dataset,ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bleu_score(predict,target):\n",
    "    try:\n",
    "        target = [[[j for index,j in enumerate(i)]] for i in target]\n",
    "        predict = [[j for index,j in enumerate(i)] for i in predict]\n",
    "        BLEUscore = nltk.translate.bleu_score.corpus_bleu(target,predict)\n",
    "    except:\n",
    "        BLEUscore = -1\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x),len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_len = [len(i) for i in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = list(filter(lambda x:x[2] < 50,sorted(zip(test_x,test_y,test_x_len),key=lambda x:x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.asarray([i[0] for i in tmp])\n",
    "test_y = np.asarray([i[1] for i in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_how _ignorant _of _you\n",
      "凭 你 这 副 德性\n",
      "_just _step _outside _.\n",
      "站 到 走廊 上 。\n",
      "_retrieve _the _weapon _.\n",
      "去取 武器 。\n",
      "_tying _?\n",
      "系 系 绳子 ？\n",
      "_leave _the _city _.\n",
      "离开 城市 。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(test_x[:1500]))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in test_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in test_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99422, 99422)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_test_loss(test_x,test_y,display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(0,len(test_x),batch_size):\n",
    "        batch_x,batch_y = test_x[j:j + batch_size],test_y[j:j + batch_size]#test_set.next_batch(batch_size)\n",
    "        if len(batch_x) < batch_size:\n",
    "            continue\n",
    "        bx = [len(m) + 1 for m in batch_x]\n",
    "        by = [len(m) + 1 for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = preprocessing.sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = preprocessing.sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        \n",
    "        tmp_loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,\n",
    "                                                     y_in:\n",
    "                                                     np.concatenate((\n",
    "                                                     np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                     ,x_len:lx,y_len:ly,\n",
    "                                                                        y_real_len:by,\n",
    "                                                                        x_real_len:bx,\n",
    "                                                                        y_max_len:max(by)\n",
    "                                                                        })\n",
    "        loss_list.append(tmp_loss)\n",
    "        tmp_acc = cal_acc(tran[:,:,0],batch_y)\n",
    "        accs.append(tmp_acc)\n",
    "        predict_list += [i for i in tran[:,:,0]]\n",
    "        target_list += [i for i in batch_y]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    return np.average(loss_list),np.average(accs),get_bleu_score(predict_list,target_list),predict_list,target_list,source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_loss,w_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(train_x[::10000],train_y[::10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.785 0.0 0\n"
     ]
    }
   ],
   "source": [
    "print(w_loss,w_acc,bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_text(x):\n",
    "    return [' '.join([ind2ch_oov.get(j,'') for j in i]) for i in x]\n",
    "def get_all_en_text(x):\n",
    "    return [' '.join([ind2en_oov.get(j,'') for j in i]) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "#                                                                np.concatenate((\n",
    "#                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "#                                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tran.shape\n",
    "i_save = 0\n",
    "j_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(i_save,j_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'OOVSUB_beamsearch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('middleresult/{}'.format(model_path)):os.mkdir('middleresult/{}'.format(model_path))\n",
    "if not os.path.exists('eval/{}'.format(model_path)):os.mkdir('eval/{}'.format(model_path))\n",
    "if not os.path.exists('val/{}'.format(model_path)):os.mkdir('val/{}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tmpindexs(train_index_set):\n",
    "    tmp_indexs,_ = train_index_set.next_batch(batch_size * batch_gen)\n",
    "    tmp_length = [len(train_x[i]) for i in tmp_indexs]\n",
    "    tmp_indexs = [i[0] for i in sorted(zip(tmp_indexs,tmp_length),key=lambda x:x[1])]\n",
    "    tmp = []\n",
    "    for i in random.sample(range(batch_gen),batch_gen):\n",
    "        tmp += tmp_indexs[i * batch_size:(i + 1) * batch_size]\n",
    "    tmp_indexs = tmp\n",
    "    return tmp_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_indexs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 loss:62.03953251976508 lr:1 0.04 % [>--------------------------------------------------] 58/154687 \t used:28s eta:75089 s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-c57c24004251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                                                 \u001b[1;33m,\u001b[0m\u001b[0my_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                                                 \u001b[0mx_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                                                 \u001b[0my_max_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                                                                })\n\u001b[1;32m     53\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 60\n",
    "restore = False\n",
    "lr = 1\n",
    "\n",
    "batch_gen = 100\n",
    "\n",
    "from utils import *\n",
    "if not restore:\n",
    "    train_index_set = Dataset(np.arange(len(train_x)),np.arange(len(train_y)))\n",
    "    tmp_indexs = []\n",
    "    \n",
    "exp_loss = None\n",
    "alpha = 0.97\n",
    "for i in range(i_save,n_epoch):\n",
    "    one_epoch = i + 1\n",
    "    i_save = i\n",
    "    worksum = int(len(train_y)/batch_size)\n",
    "    pb = ProgressBar(worksum=worksum)\n",
    "    pb.startjob()\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for j in range(worksum):\n",
    "        one_batch = j\n",
    "        if restore == True and j < j_save:\n",
    "            pb.finishsum += 1\n",
    "            continue\n",
    "        restore = False\n",
    "        \n",
    "        j_save = j\n",
    "        \n",
    "        if tmp_indexs == []:\n",
    "            tmp_indexs = get_tmpindexs(train_index_set)\n",
    "        batch_indexs,tmp_indexs = tmp_indexs[:batch_size],tmp_indexs[batch_size:]\n",
    "        batch_x,batch_y = train_x[batch_indexs],train_y[batch_indexs]\n",
    "\n",
    "        bx = [min(len(m) + 1,seq_max_len) for m in batch_x]\n",
    "        by = [min(len(m) + 1,seq_max_len) for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = preprocessing.sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = preprocessing.sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        #print(batch_x.shape,batch_y.shape)\n",
    "        \n",
    "        _, loss = session.run([optimizer,train_loss],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "                                                                np.concatenate((\n",
    "                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                                ,y_real_len:by,\n",
    "                                                                x_real_len:bx,\n",
    "                                                                y_max_len:max(by)\n",
    "                                                               })\n",
    "        train_loss_list.append(loss)\n",
    "        #tmp_train_acc = cal_acc(tran,batch_y)\n",
    "        #train_acc_list.append(tmp_train_acc)\n",
    "        exp_loss = loss if exp_loss == None else alpha * exp_loss + (1 - alpha) * loss\n",
    "        pb.info = \"iter {} loss:{} lr:{}\".format(i + 1,exp_loss,lr)\n",
    "        with open('val/{}/train_loss.txt'.format(model_path),'a') as whdl:\n",
    "            whdl.write(\"{}\\t{}\\t{}\\n\".format(one_epoch,one_batch,loss))\n",
    "        val_step = int(worksum / 4)\n",
    "        if j % val_step == 0 and j != 0:\n",
    "            test_loss,test_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(test_x[::4],test_y[::4])\n",
    "            _,train_acc,train_bleu_score,train_predict_list,train_target_list,train_source_list = calc_test_loss(train_x[::1000],train_y[::1000])\n",
    "            predict_texts = get_all_text(predict_list)\n",
    "            target_texts = get_all_text(target_list)\n",
    "            source_texts = get_all_en_text(source_list)\n",
    "            \n",
    "            train_predict_texts = get_all_text(train_predict_list)\n",
    "            train_target_texts = get_all_text(train_target_list)\n",
    "            train_source_texts = get_all_en_text(train_source_list)\n",
    "            \n",
    "            with open('eval/{}/{}_{}_predict'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "                    \n",
    "            with open('eval/{}/{}_{}_predict_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            print(\"\\niter {} step {} train loss {} train acc {} test loss {} test acc {} bleu {} lr {}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            with open('val/{}/test_loss.txt'.format(model_path),'a') as whdl:\n",
    "                whdl.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            try:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session,'middleresult/{}/result_{}_{}'.format(model_path,i + 1,j))\n",
    "            except:\n",
    "                print('save fail')\n",
    "        lr_step = int(worksum / 2) - 1\n",
    "        if j % lr_step == 0 and j != 0:\n",
    "            if (i + 1) >= 6:\n",
    "                lr = lr / 2\n",
    "        pb.complete(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
