{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import tflearn\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "with open('middleresult/en_ch_35word.pkl','wb') as whdl:\n",
    "    pickle.dump((\n",
    "        train_x,\n",
    "        test_x,\n",
    "        train_y,\n",
    "        test_y,\n",
    "        ind2ch,\n",
    "        ch2ind,\n",
    "        ind2en,\n",
    "        en2ind,\n",
    "    ),whdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/preprocessing_tokenlizer/sentence_tokened_by_word.pkl','rb') as fhdl:\n",
    "    (\n",
    "         ind2ch,\n",
    "         ch2ind,\n",
    "         ind2en,\n",
    "         en2ind,\n",
    "         train_x,\n",
    "         train_y,\n",
    "    ) = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/preprocessing_subword/subwords_allwords.en','rb') as fhdl:\n",
    "    en_subword = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_subword_dic = dict(zip(en_subword['origin'],en_subword['segmented']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('middleresult/char/zh_vocab.txt',encoding='utf-8') as fhdl:\n",
    "    ch_subwords = [line.strip().split(\"\\t\") for line in fhdl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(661850, 406740)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind2ch),len(ind2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_inv_size_base = 40000#len(ind2en) + 3\n",
    "target_inv_size_base = 40000#len(ind2ch) + 3\n",
    "\n",
    "USE_GPU = 0\n",
    "\n",
    "attention_hidden_size = 1024\n",
    "attention_output_size = 1024\n",
    "embedding_size = 1024\n",
    "seq_max_len = 60\n",
    "num_units = 1024\n",
    "batch_size = 64\n",
    "layer_number = 2\n",
    "max_grad = 1.0\n",
    "dropout = 0.2\n",
    "sentence_max_length = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_inv = list(map(lambda x:x[0],sorted(ch2ind.items(),key=lambda x:x[1])[:target_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_inv = list(map(lambda x:x[0],sorted(en2ind.items(),key=lambda x:x[1])[:src_inv_size_base]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_inv = en_inv[:3] + ['_' + i for i in en_inv[3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch_inv_tmpdic = dict(zip(ch_inv,range(len(ch_inv))))\n",
    "ch_oov = [i[0] for i in ch_subwords if i[0] not in ch_inv_tmpdic]\n",
    "en_inv_tmpdic = dict(zip(en_inv,range(len(en_inv))))\n",
    "en_oov = [i for i in en_subword['subwords'] if i not in en_inv_tmpdic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2ch_oov = dict(zip(range(len(ch_oov) + len(ch_inv)),ch_inv + ch_oov))\n",
    "ch2ind_oov = dict(zip(ch_inv + ch_oov,range(len(ch_oov) + len(ch_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind2en_oov = dict(zip(range(len(en_oov) + len(en_inv)),en_inv + en_oov))\n",
    "en2ind_oov = dict(zip(en_inv + en_oov,range(len(en_oov) + len(en_inv))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5846, 3466)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch_oov),len(en_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = src_inv_size_base + len(en_oov)\n",
    "target_vocat_size = target_inv_size_base + len(ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43466, 43466, 45846, 45846)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size,len(ind2en_oov),target_vocat_size,len(ind2ch_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 10000000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_x = np.asarray(train_x)\n",
    "train_y = np.asarray(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2360, 2360)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ind['james'],en2ind_oov['_james']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = [i[::-1] for i in train_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_x = sequence.pad_sequences(train_x,seq_max_len,padding='post',value=en2ind['<eos>'])\n",
    "#train_y = sequence.pad_sequences(train_y,seq_max_len,padding='post',value=ch2ind['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is something so terrible , so impropriety\n",
      "我 做 得 很 糟糕 ， 很 不得体 的 事\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0,len(train_x))\n",
    "print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_x,train_y , test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def en_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < src_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            en_word = ind2en[wordind]\n",
    "            if en_word not in en_subword_dic:\n",
    "                sentence_inds.append(en2ind_oov['<unk>'])\n",
    "            en_pieces = en_subword_dic[en_word]\n",
    "            pieces_index = [en2ind_oov[i] for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ch_ind2ind(sentence):\n",
    "    sentence_inds = []\n",
    "    for wordind in sentence:\n",
    "        if wordind < target_inv_size_base:\n",
    "            sentence_inds.append(wordind)\n",
    "        else:\n",
    "            ch_word = ind2ch[wordind]\n",
    "            en_pieces = [i for i in ch_word]\n",
    "            pieces_index = [ch2ind_oov.get(i,ch2ind_oov['<unk>']) for i in en_pieces]\n",
    "            sentence_inds += pieces_index\n",
    "    return sentence_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_tmp = []\n",
    "for index,sentence in enumerate(train_x):\n",
    "    train_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y_tmp = []\n",
    "for index,sentence in enumerate(train_y):\n",
    "    train_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_tmp = []\n",
    "for index,sentence in enumerate(test_x):\n",
    "    test_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_y_tmp = []\n",
    "for index,sentence in enumerate(test_y):\n",
    "    test_y_tmp.append(ch_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x_tmp)\n",
    "train_y = np.asarray(train_y_tmp)\n",
    "test_x = np.asarray(test_x_tmp)\n",
    "test_y = np.asarray(test_y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x_tmp\n",
    "del train_y_tmp\n",
    "del test_x_tmp\n",
    "del test_y_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_probably _made _a _thousand _calls _on _behalf _of _the _mission _.\n",
      "差不多 打 了 有 上 千个 电话 。\n",
      "_invented _by _the _earl _of _sandwich _? _with _something _special _added _.\n",
      "三明治 伯爵 发明 的 ？ 还 加 料 了 。\n",
      "_i _rob _the _shit _out _that _motherfucker _.\n",
      "我 就 瞒 天 过 海 来 这 拿 点儿 小钱 。\n",
      "_yan _x ia oh ong _, _deputy _director _of _the _national _copyright _administration _, _told _reporters _in _beijing _that _the _government _regards _shenzhen _proview _technology _as _the _rightful _owner _of _the _trademark _for _the _popular _tablet _computers _.\n",
      "平板 电脑 目前 广 受欢迎 ， 而 中国 国 家 版 权 局 副局长 阎 晓 宏 告诉 位于 北京 的 记者 ， 政府 认为 深圳 唯 冠 科技 是 其 商标 的 合法 所有者 。\n",
      "_other _infectious _routes _including _int ram usc ular _and _oral _inoculation _also _could _induce _lung _function _abnormality _but _with _a _delay _onset _. _immun oh ist oc hem ical _staining _showed _the _numbers _of _cat ech ol amin erg ic _neurons _in _the _mid br ain _were _gradually _declined _after _infection _.\n",
      "其他 的 感染 途径 如 肌 肉 注 射 或 口 内 食 感染 ， 同样 会 造成 小鼠 感染 并 出现 肺 功能 的 异常 ， 但是 发病 的 时间 与 中枢神经 被 波及 有关 。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(train_x))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in train_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9900000, 100000, 9900000, 100000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(test_x),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:{}'.format(USE_GPU)):\n",
    "    #initializer = tf.random_uniform_initializer(\n",
    "    #    -0.08, 0.08)\n",
    "    initializer = tf.truncated_normal_initializer(\n",
    "        mean=0.0,stddev=0.02)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    x_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_max_len = tf.placeholder(tf.int32, shape=[])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "    \n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # encoder\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    cell_list = []\n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "    cell_list = []\n",
    "    \n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_backword_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_backword_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell,encoder_backword_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len,dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        encoder_state = tuple(encoder_state)\n",
    "    \n",
    "    # decoder \n",
    "    #decoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    cell_list = []\n",
    "    for i in range(layer_number):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        decoder_cell = cell_list[0]\n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    # Helper\n",
    "    \n",
    "    # attention\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        attention_hidden_size, encoder_outputs,\n",
    "        memory_sequence_length=x_real_len,scale=True)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=attention_output_size)\n",
    "    \n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        target_vocat_size, use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp,sequence_length= y_len)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "       \n",
    "        outputs, _,___  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_real_len, y_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        helper_predict = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_decoder,\n",
    "            tf.fill([batch_size], ch2ind['<go>']), ch2ind['<eos>'])\n",
    "        decoder_predict = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper_predict, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "        outputs_predict,_, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_predict, maximum_iterations=sentence_max_length)\n",
    "    translations = outputs_predict.sample_id\n",
    "\n",
    "    # calculate loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        batch_size)\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(logits,target):\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target[:,:seq_max_len], logits[:,:seq_max_len]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saver.restore(session,'middleresult/align/result_1_20847')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middleresult/result_deep'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'middleresult/result_deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saver.save(session,'middleresult/result_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import Dataset,ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bleu_score(predict,target):\n",
    "    try:\n",
    "        target = [[[j for index,j in enumerate(i)]] for i in target]\n",
    "        predict = [[j for index,j in enumerate(i)] for i in predict]\n",
    "        BLEUscore = nltk.translate.bleu_score.corpus_bleu(target,predict)\n",
    "    except:\n",
    "        BLEUscore = -1\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_x),len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_len = [len(i) for i in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = list(filter(lambda x:x[2] < 50,sorted(zip(test_x,test_y,test_x_len),key=lambda x:x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = np.asarray([i[0] for i in tmp])\n",
    "test_y = np.asarray([i[1] for i in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_justice _estimates\n",
      "司法部门 评估 认为\n",
      "_she _battled _incontinence _.\n",
      "她 与 大小便 失禁 做 斗争 。\n",
      "_kid _. _enough _.\n",
      "孩子 。 够 了 。\n",
      "_you _need _to _enter\n",
      "你 需要 进入\n",
      "_my _feet _are _swollen\n",
      "在 琥珀 里 站 了 这么久\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(test_x[:1500]))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in test_x[index]]))\n",
    "    print(' '.join([ind2ch_oov.get(i,'') for i in test_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99422, 99422)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_test_loss(test_x,test_y,display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(0,len(test_x),batch_size):\n",
    "        batch_x,batch_y = test_x[j:j + batch_size],test_y[j:j + batch_size]#test_set.next_batch(batch_size)\n",
    "        if len(batch_x) < batch_size:\n",
    "            continue\n",
    "        bx = [len(m) + 1 for m in batch_x]\n",
    "        by = [len(m) + 1 for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        \n",
    "        tmp_loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,\n",
    "                                                     y_in:\n",
    "                                                     np.concatenate((\n",
    "                                                     np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                     ,x_len:lx,y_len:ly,\n",
    "                                                                        y_real_len:by,\n",
    "                                                                        x_real_len:bx,\n",
    "                                                                        y_max_len:max(by)\n",
    "                                                                        })\n",
    "        loss_list.append(tmp_loss)\n",
    "        tmp_acc = cal_acc(tran,batch_y)\n",
    "        accs.append(tmp_acc)\n",
    "        predict_list += [i for i in tran]\n",
    "        target_list += [i for i in batch_y]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    return np.average(loss_list),np.average(accs),get_bleu_score(predict_list,target_list),predict_list,target_list,source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 15/15 \t used:13s eta:0 s"
     ]
    }
   ],
   "source": [
    "w_loss,w_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(train_x[::10000],train_y[::10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-48d7747c8c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbleu_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'w_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(w_loss,w_acc,bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_text(x):\n",
    "    return [' '.join([ind2ch_oov.get(j,'') for j in i]) for i in x]\n",
    "def get_all_en_text(x):\n",
    "    return [' '.join([ind2en_oov.get(j,'') for j in i]) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "#                                                                np.concatenate((\n",
    "#                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "#                                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tran.shape\n",
    "i_save = 0\n",
    "j_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(i_save,j_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'OOVSUB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('middleresult/{}'.format(model_path)):os.mkdir('middleresult/{}'.format(model_path))\n",
    "if not os.path.exists('eval/{}'.format(model_path)):os.mkdir('eval/{}'.format(model_path))\n",
    "if not os.path.exists('val/{}'.format(model_path)):os.mkdir('val/{}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tmpindexs(train_index_set):\n",
    "    tmp_indexs,_ = train_index_set.next_batch(batch_size * batch_gen)\n",
    "    tmp_length = [len(train_x[i]) for i in tmp_indexs]\n",
    "    tmp_indexs = [i[0] for i in sorted(zip(tmp_indexs,tmp_length),key=lambda x:x[1])]\n",
    "    tmp = []\n",
    "    for i in random.sample(range(batch_gen),batch_gen):\n",
    "        tmp += tmp_indexs[i * batch_size:(i + 1) * batch_size]\n",
    "    tmp_indexs = tmp\n",
    "    return tmp_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_indexs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6 loss:55.007703761972934 lr:0.5 39.29 % [===================>-------------------------------] 60778/154687 \t used:9s eta:14 s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-f85c141bb66e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m                                                                 \u001b[1;33m,\u001b[0m\u001b[0my_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                                                 \u001b[0mx_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                                                                 \u001b[0my_max_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                                                                })\n\u001b[1;32m     53\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\program files\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 60\n",
    "restore = True\n",
    "lr = 1 / 2\n",
    "\n",
    "batch_gen = 100\n",
    "\n",
    "from utils import *\n",
    "if not restore:\n",
    "    train_index_set = Dataset(np.arange(len(train_x)),np.arange(len(train_y)))\n",
    "    tmp_indexs = []\n",
    "    \n",
    "exp_loss = None\n",
    "alpha = 0.97\n",
    "for i in range(i_save,n_epoch):\n",
    "    one_epoch = i + 1\n",
    "    i_save = i\n",
    "    worksum = int(len(train_y)/batch_size)\n",
    "    pb = ProgressBar(worksum=worksum)\n",
    "    pb.startjob()\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for j in range(worksum):\n",
    "        one_batch = j\n",
    "        if restore == True and j < j_save:\n",
    "            pb.finishsum += 1\n",
    "            continue\n",
    "        restore = False\n",
    "        \n",
    "        j_save = j\n",
    "        \n",
    "        if tmp_indexs == []:\n",
    "            tmp_indexs = get_tmpindexs(train_index_set)\n",
    "        batch_indexs,tmp_indexs = tmp_indexs[:batch_size],tmp_indexs[batch_size:]\n",
    "        batch_x,batch_y = train_x[batch_indexs],train_y[batch_indexs]\n",
    "\n",
    "        bx = [min(len(m) + 1,seq_max_len) for m in batch_x]\n",
    "        by = [min(len(m) + 1,seq_max_len) for m in batch_y]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        ly = [max(by)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        batch_y = sequence.pad_sequences(batch_y,max(by),padding='post',value=ch2ind_oov['<eos>'])\n",
    "        #print(batch_x.shape,batch_y.shape)\n",
    "        \n",
    "        _, loss = session.run([optimizer,train_loss],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "                                                                np.concatenate((\n",
    "                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                                ,y_real_len:by,\n",
    "                                                                x_real_len:bx,\n",
    "                                                                y_max_len:max(by)\n",
    "                                                               })\n",
    "        train_loss_list.append(loss)\n",
    "        #tmp_train_acc = cal_acc(tran,batch_y)\n",
    "        #train_acc_list.append(tmp_train_acc)\n",
    "        exp_loss = loss if exp_loss == None else alpha * exp_loss + (1 - alpha) * loss\n",
    "        pb.info = \"iter {} loss:{} lr:{}\".format(i + 1,exp_loss,lr)\n",
    "        with open('val/{}/train_loss.txt'.format(model_path),'a') as whdl:\n",
    "            whdl.write(\"{}\\t{}\\t{}\\n\".format(one_epoch,one_batch,loss))\n",
    "        val_step = int(worksum / 4)\n",
    "        if j % val_step == 0 and j != 0:\n",
    "            lr = lr / 2\n",
    "            test_loss,test_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(test_x[::4],test_y[::4])\n",
    "            _,train_acc,train_bleu_score,train_predict_list,train_target_list,train_source_list = calc_test_loss(train_x[::1000],train_y[::1000])\n",
    "            predict_texts = get_all_text(predict_list)\n",
    "            target_texts = get_all_text(target_list)\n",
    "            source_texts = get_all_en_text(source_list)\n",
    "            \n",
    "            train_predict_texts = get_all_text(train_predict_list)\n",
    "            train_target_texts = get_all_text(train_target_list)\n",
    "            train_source_texts = get_all_en_text(train_source_list)\n",
    "            \n",
    "            with open('eval/{}/{}_{}_predict'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "                    \n",
    "            with open('eval/{}/{}_{}_predict_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            print(\"\\niter {} step {} train loss {} train acc {} test loss {} test acc {} bleu {} lr {}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            with open('val/{}/test_loss.txt'.format(model_path),'a') as whdl:\n",
    "                whdl.write(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            try:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session,'middleresult/{}/result_{}_{}'.format(model_path,i + 1,j))\n",
    "            except:\n",
    "                print('save fail')\n",
    "        lr_step = int(worksum / 2) - 1\n",
    "        if j % lr_step == 0 and j != 0:\n",
    "            if (i + 1) >= 6:\n",
    "                lr = lr / 2\n",
    "        pb.complete(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# predict the real shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from middleresult/OOVSUB/result_3_116013\n"
     ]
    }
   ],
   "source": [
    "saver.restore(session,'middleresult/OOVSUB/result_3_116013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from xml.etree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_lines = open('./ai_challenger_translation_test_a_20170923/ai_challenger_translation_test_a_20170923.sgm',encoding='utf-8').read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<seg id=\"283\"> The room includes two 32-inch flat screen TV's, sink and microwave & refrigerator. </seg>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 13)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(validation_lines[286]),validation_lines[286].find('>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_text = []\n",
    "for line in validation_lines[4:-4]:\n",
    "    firstpos = line.find('>')\n",
    "    lastpos = -6\n",
    "    validate_text.append(line[firstpos + 1:lastpos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\jiaohuan\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.917 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "for index,val in enumerate(validate_text):\n",
    "    validate_text[index] = [i.lower() for i in jieba.cut(val) if i.strip()]a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'room', 'includes', 'two', '32', '-', 'inch', 'flat', 'screen', 'tv', \"'\", 's', ',', 'sink', 'and', 'microwave', '&', 'refrigerator', '.']\n"
     ]
    }
   ],
   "source": [
    "print(validate_text[282])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_x = []\n",
    "for index,sentence in enumerate(validate_text):\n",
    "    validate_x.append([en2ind.get(word,en2ind['<unk>']) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_x = np.array(validate_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_x_tmp = []\n",
    "for index,sentence in enumerate(validate_x):\n",
    "    validate_x_tmp.append(en_ind2ind(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_i _leave _in _the _morning _. _i _work _three _days _. _call _me _.\n",
      "_here _' _s _to _ant ig ra v ity _. _stay _with _me _, _buddy _. _you _' _ll _get _the _spins _.\n",
      "_i _hear _they _send _people _to _cut _you _off _at _the _knees\n",
      "_but _the _secret _is _, _you _grind _it _from _beans _, _not _crap _.\n",
      "_hey _, _where _can _a _man _and _his _missus _get _something _to _eat _around _here _?\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(validate_x_tmp))\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in validate_x_tmp[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_x = np.asarray(validate_x_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([np.sum(np.asarray(np.asarray(x) == en2ind['<unk>'],dtype=np.int)) for x in validate_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_translation(test_x,display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(0,len(test_x),batch_size):\n",
    "        batch_x = test_x[j:j + batch_size]\n",
    "        while len(batch_x) < batch_size:\n",
    "            batch_x = np.concatenate([batch_x,batch_x],axis=0)\n",
    "        batch_x = batch_x[:batch_size]\n",
    "        bx = [len(m) + 1 for m in batch_x]\n",
    "        \n",
    "        lx = [max(bx)] * batch_size\n",
    "        \n",
    "        batch_x = sequence.pad_sequences(batch_x,max(bx),padding='post',value=en2ind_oov['<eos>'])\n",
    "        \n",
    "        tran = session.run(translations,feed_dict={x:batch_x\n",
    "                                                     ,x_len:lx,\n",
    "                                                     x_real_len:bx,\n",
    "                                                     })\n",
    "        predict_list += [i for i in tran]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    predict_list = predict_list[:len(test_x)]\n",
    "    return predict_list,source_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 125/125 \t used:36s eta:0 s"
     ]
    }
   ],
   "source": [
    "validate_predict,___ = get_translation(validate_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_predict_tmp = []\n",
    "for line in validate_predict:\n",
    "    tmp = []\n",
    "    for word in line:\n",
    "        if word == ch2ind['<eos>']:\n",
    "            break\n",
    "        tmp.append(word)\n",
    "    validate_predict_tmp.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_predict_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_predict = validate_predict_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4420\n",
      "_oh _, _sure _, _all _we _have _to _do _is _escape _from _this _cell _, _right _?\n",
      "当然，我们只要从这个牢房里逃走，对吗？\n",
      "3787\n",
      "_i _hope _you _' _re _not _going _to _keep _on _about _it _.\n",
      "希望你不会继续谈这件事。\n",
      "4887\n",
      "_the _germans _do _not _act _like _us _, _neither _do _they _think _like _us _...\n",
      "德国人不像我们一样，他们也不像我们这样的人…\n",
      "7890\n",
      "_if _that _was _my _wife _, _i _know _where _i _' _d _be _.\n",
      "如果那是我妻子，我知道我在哪里。\n",
      "4350\n",
      "_atop _the _pantheon _of _explorers _! _he _' _d _be _able _to _experience\n",
      "在探险家的神殿里！他将能体验到\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(validate_predict))\n",
    "    print(index)\n",
    "    print(' '.join([ind2en_oov.get(i,'') for i in validate_x[index]]))\n",
    "    print(''.join([ind2ch_oov.get(i,'') for i in validate_predict[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_result = []\n",
    "for index in range(len(validate_predict)):\n",
    "    one = ''.join([ind2ch_oov.get(i,'') for i in validate_predict[index]])\n",
    "    validate_result.append(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['所以我不会在你面前谈论这个。',\n",
       " '与那些可以帮助他们和金钱的人联系的人联系。',\n",
       " '已经在他们身上了。早一点。来吧。坐下。',\n",
       " '在对酸性反应的反应中变氧化。',\n",
       " '我知道，我也希望能证明，但我不能。',\n",
       " '我不知道！他表现得很搞笑然后开始咳血。',\n",
       " '几位工作人员的成员都可以确认我在那里',\n",
       " '我很高兴你能来陪我，亲爱的。',\n",
       " '先生，这是警察行动。我要谈谈',\n",
       " '很明显是那个撞方向盘的人。让我猜猜。那父亲是喝醉了。']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_result[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! mkdir answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('answer/9-26.txt','w',encoding='utf-8') as whdl:\n",
    "    for line in validate_result:\n",
    "        whdl.write(\"{}\\n\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
